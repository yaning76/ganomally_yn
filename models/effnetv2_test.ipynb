{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "opened-ghost",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "balanced-oasis",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    DCGAN ENCODER NETWORK\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, imageSize, nz, nc, ngf, ngpu, n_extra_layers=0, add_final_conv=True):\n",
    "        # nz : dimensionality of the latent space潜在空间的维度\n",
    "        # nc : number of image channels\n",
    "        # ndf : channels of middle layers for generator 生成器中间层通道数\n",
    "        # ngpu : number of gpu\n",
    "        # n_extra_layers : extra layers of Encoder and Decoder\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        assert imageSize % 16 == 0, \"imageSize has to be a multiple of 16\"\n",
    "\n",
    "        main = nn.Sequential()\n",
    "        # input is nc x imageSize x imageSize\n",
    "        main.add_module('initial-conv-{0}-{1}'.format(nc, ngf),\n",
    "                        nn.Conv2d(nc, ngf, 4, 2, 1, bias=False))\n",
    "        main.add_module('initial-relu-{0}'.format(ngf),\n",
    "                        nn.LeakyReLU(0.2, inplace=True))\n",
    "        csize, cndf = imageSize / 2, ngf\n",
    "\n",
    "        # Extra layers\n",
    "        for t in range(n_extra_layers):\n",
    "            main.add_module('extra-layers-{0}-{1}-conv'.format(t, cndf),\n",
    "                            nn.Conv2d(cndf, cndf, 3, 1, 1, bias=False))\n",
    "            main.add_module('extra-layers-{0}-{1}-batchnorm'.format(t, cndf),\n",
    "                            nn.BatchNorm2d(cndf))\n",
    "            main.add_module('extra-layers-{0}-{1}-relu'.format(t, cndf),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "        while csize > 4:\n",
    "            in_feat = cndf\n",
    "            out_feat = cndf * 2\n",
    "            main.add_module('pyramid-{0}-{1}-conv'.format(in_feat, out_feat),\n",
    "                            nn.Conv2d(in_feat, out_feat, 4, 2, 1, bias=False))\n",
    "            main.add_module('pyramid-{0}-batchnorm'.format(out_feat),\n",
    "                            nn.BatchNorm2d(out_feat))\n",
    "            main.add_module('pyramid-{0}-relu'.format(out_feat),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "            cndf = cndf * 2\n",
    "            csize = csize / 2\n",
    "\n",
    "        # state size. K x 4 x 4\n",
    "        if add_final_conv:\n",
    "            main.add_module('final-{0}-{1}-conv'.format(cndf, 1),\n",
    "                            nn.Conv2d(cndf, nz, 4, 1, 0, bias=False))\n",
    "\n",
    "        self.main = main\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class NetD(nn.Module):\n",
    "    \"\"\"\n",
    "    DISCRIMINATOR NETWORK\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(NetD, self).__init__()\n",
    "        model = Encoder(imageSize=128,nz=1,nc=3,ngf=64,ngpu=1,n_extra_layers=0)\n",
    "        layers = list(model.main.children())\n",
    "\n",
    "        self.features = nn.Sequential(*layers[:-1])\n",
    "        self.classifier = nn.Sequential(layers[-1])\n",
    "        self.classifier.add_module('Sigmoid', nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        features = features\n",
    "        classifier = self.classifier(features)\n",
    "        print(classifier.shape)\n",
    "        classifier = classifier.view(-1, 1).squeeze(1)\n",
    "\n",
    "        return classifier, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "native-excuse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "model=NetD()\n",
    "input=torch.rand((1,3,128,128))\n",
    "classifier, features=model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "enabling-glossary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3456], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "narrow-twins",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 4, 4])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-estimate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "geological-alias",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    \"\"\"\n",
    "    This function is taken from the original tf repo.\n",
    "    It ensures that all layers have a channel number that is divisible by 8\n",
    "    It can be seen here:\n",
    "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    :param v:\n",
    "    :param divisor:\n",
    "    :param min_value:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "def conv_3x3_bn(inp, oup, stride):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        SiLU()\n",
    "    )\n",
    "if hasattr(nn, 'SiLU'):\n",
    "    SiLU = nn.SiLU\n",
    "else:\n",
    "    # For compatibility with old PyTorch versions\n",
    "    class SiLU(nn.Module):\n",
    "        def forward(self, x):\n",
    "            return x * torch.sigmoid(x)\n",
    "        \n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, inp, oup, reduction=4):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "                nn.Linear(oup, _make_divisible(inp // reduction, 8)),\n",
    "                SiLU(),\n",
    "                nn.Linear(_make_divisible(inp // reduction, 8), oup),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y\n",
    "        \n",
    "class MBConv(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, expand_ratio, use_se):\n",
    "        super(MBConv, self).__init__()\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        hidden_dim = round(inp * expand_ratio)\n",
    "        self.identity = stride == 1 and inp == oup\n",
    "        if use_se:\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw\n",
    "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                SiLU(),\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                SiLU(),\n",
    "                SELayer(inp, hidden_dim),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                # fused\n",
    "                nn.Conv2d(inp, hidden_dim, 3, stride, 1, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                SiLU(),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.identity:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "\n",
    "            return self.conv(x)\n",
    "        \n",
    "class NetD(nn.Module):\n",
    "    \"\"\"\n",
    "    DISCRIMINATOR NETWORK\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(NetD, self).__init__()\n",
    "#         self.conv1=conv_3x3_bn(inp=3, oup=24, stride=2)\n",
    "#         self.Fused_MBConv_0=MBConv(inp=24, oup=24, stride=1, expand_ratio=1, use_se=0)\n",
    "#         self.Fused_MBConv_1=MBConv(inp=24, oup=48, stride=2, expand_ratio=4, use_se=0)\n",
    "#         self.Fused_MBConv_2=MBConv(inp=48, oup=64, stride=2, expand_ratio=4, use_se=0)\n",
    "        \n",
    "#         self.MBConv_3=MBConv(inp=64, oup=128, stride=2, expand_ratio=4, use_se=1)\n",
    "#         self.MBConv_4=MBConv(inp=128, oup=160, stride=2, expand_ratio=6, use_se=1)\n",
    "#         self.final=nn.Sequential(nn.Conv2d(160, 1, 4, 1, 0, bias=False),\n",
    "#                                  nn.Sigmoid())\n",
    "\n",
    "        \n",
    "        self.conv1=conv_3x3_bn(inp=3, oup=64, stride=2)\n",
    "        self.Fused_MBConv_0=MBConv(inp=64, oup=64, stride=1, expand_ratio=1, use_se=0)\n",
    "        self.Fused_MBConv_1=MBConv(inp=64, oup=128, stride=2, expand_ratio=4, use_se=0)\n",
    "        self.Fused_MBConv_2=MBConv(inp=128, oup=256, stride=2, expand_ratio=4, use_se=0)\n",
    "        \n",
    "        self.MBConv_3=MBConv(inp=256, oup=512, stride=2, expand_ratio=4, use_se=1)\n",
    "        self.MBConv_4=MBConv(inp=512, oup=1024, stride=2, expand_ratio=6, use_se=1)\n",
    "        self.MBConv_5=MBConv(inp=1024, oup=2048, stride=2, expand_ratio=6, use_se=1)\n",
    "        self.final=nn.Sequential(nn.Conv2d(2048, 1, 4, 1, 0, bias=False),\n",
    "                                 nn.Sigmoid())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)#[1, 64, 64, 64]\n",
    "        print(x.shape)\n",
    "        x = self.Fused_MBConv_0(x)#[1, 64, 64, 64]\n",
    "        print(x.shape)\n",
    "        x = self.Fused_MBConv_1(x)#1, 128, 32, 32\n",
    "        print(x.shape)\n",
    "        x=self.Fused_MBConv_2(x)#1, 256, 16, 16\n",
    "        print(x.shape)\n",
    "        x = self.MBConv_3(x)#1, 256, 16, 16\n",
    "        print(x.shape)\n",
    "        x = self.MBConv_4(x)\n",
    "        features = self.MBConv_5(x)#1, 512, 8, 8\n",
    "        print(features.shape)\n",
    "        classifier = self.final(features)#1, 1024, 4, 4\n",
    "        print(classifier.shape)\n",
    "        classifier = classifier.view(-1, 1).squeeze(1)#[1, 1, 1, 1]\n",
    "\n",
    "        return classifier, features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "devoted-yacht",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 128, 128])\n",
      "torch.Size([1, 64, 128, 128])\n",
      "torch.Size([1, 128, 64, 64])\n",
      "torch.Size([1, 256, 32, 32])\n",
      "torch.Size([1, 512, 16, 16])\n",
      "torch.Size([1, 2048, 4, 4])\n",
      "torch.Size([1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "model=NetD()\n",
    "input=torch.rand((1,3,256,256))\n",
    "classifier, features=model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "artificial-network",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3511], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "hybrid-afternoon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 8, 8])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-yield",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "japanese-drinking",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olive-nutrition",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-transcription",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "designing-penetration",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(nn, 'SiLU'):\n",
    "    SiLU = nn.SiLU\n",
    "else:\n",
    "    # For compatibility with old PyTorch versions\n",
    "    class SiLU(nn.Module):\n",
    "        def forward(self, x):\n",
    "            return x * torch.sigmoid(x)\n",
    "        \n",
    "class MBConv(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, expand_ratio, use_se):\n",
    "        super(MBConv, self).__init__()\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        hidden_dim = round(inp * expand_ratio)\n",
    "        self.identity = stride == 1 and inp == oup\n",
    "        if use_se:\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw\n",
    "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                SiLU(),\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                SiLU(),\n",
    "                SELayer(inp, hidden_dim),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                # fused\n",
    "                nn.Conv2d(inp, hidden_dim, 3, stride, 1, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                SiLU(),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.identity:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "\n",
    "            return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "hydraulic-seller",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t（）, c（通道数）, n(重复数), s（stride）, SE\n",
    "# [1,  24,  2, 1, 0],\n",
    "# [4,  48,  4, 2, 0],\n",
    "# [4,  64,  4, 2, 0],\n",
    "# [4, 128,  6, 2, 1],\n",
    "# [6, 160,  9, 1, 1],\n",
    "# [6, 256, 15, 2, 1],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "caring-cache",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=MBConv(inp=24, oup=24, stride=2, expand_ratio=1, use_se=0)\n",
    "input=torch.rand((1,24,64,64))\n",
    "out1=model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afraid-breath",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 24, 32, 32])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aggregate-sharing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_3x3_bn(inp, oup, stride):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        SiLU()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "rural-correspondence",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=conv_3x3_bn(3, 24, 2)\n",
    "input=torch.rand((1,3,128,128))\n",
    "out1=model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "second-poker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 24, 64, 64])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respected-mobility",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blocked-cleanup",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-nothing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-thumbnail",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subject-generation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "__all__ = ['effnetv2_s', 'effnetv2_m', 'effnetv2_l', 'effnetv2_xl']\n",
    "\n",
    "\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    \"\"\"\n",
    "    This function is taken from the original tf repo.\n",
    "    It ensures that all layers have a channel number that is divisible by 8\n",
    "    It can be seen here:\n",
    "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    :param v:\n",
    "    :param divisor:\n",
    "    :param min_value:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "\n",
    "# SiLU (Swish) activation function\n",
    "if hasattr(nn, 'SiLU'):\n",
    "    SiLU = nn.SiLU\n",
    "else:\n",
    "    # For compatibility with old PyTorch versions\n",
    "    class SiLU(nn.Module):\n",
    "        def forward(self, x):\n",
    "            return x * torch.sigmoid(x)\n",
    "        \n",
    "        \n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, inp, oup, reduction=4):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "                nn.Linear(oup, _make_divisible(inp // reduction, 8)),\n",
    "                SiLU(),\n",
    "                nn.Linear(_make_divisible(inp // reduction, 8), oup),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y\n",
    "\n",
    "\n",
    "def conv_3x3_bn(inp, oup, stride):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        SiLU()\n",
    "    )\n",
    "\n",
    "\n",
    "def conv_1x1_bn(inp, oup):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        SiLU()\n",
    "    )\n",
    "\n",
    "\n",
    "class MBConv(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, expand_ratio, use_se):\n",
    "        super(MBConv, self).__init__()\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        hidden_dim = round(inp * expand_ratio)\n",
    "        self.identity = stride == 1 and inp == oup\n",
    "        if use_se:\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw\n",
    "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                SiLU(),\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                SiLU(),\n",
    "                SELayer(inp, hidden_dim),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                # fused\n",
    "                nn.Conv2d(inp, hidden_dim, 3, stride, 1, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                SiLU(),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.identity:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class EffNetV2(nn.Module):\n",
    "    def __init__(self, cfgs, num_classes=1000, width_mult=1.):\n",
    "        super(EffNetV2, self).__init__()\n",
    "        self.cfgs = cfgs\n",
    "\n",
    "        # building first layer\n",
    "        input_channel = _make_divisible(24 * width_mult, 8)\n",
    "        layers = [conv_3x3_bn(3, input_channel, 2)]\n",
    "        # building inverted residual blocks\n",
    "        block = MBConv\n",
    "        for t, c, n, s, use_se in self.cfgs:\n",
    "            output_channel = _make_divisible(c * width_mult, 8)\n",
    "            for i in range(n):\n",
    "                layers.append(block(input_channel, output_channel, s if i == 0 else 1, t, use_se))\n",
    "                input_channel = output_channel\n",
    "        self.features = nn.Sequential(*layers)\n",
    "        # building last several layers\n",
    "        output_channel = _make_divisible(1792 * width_mult, 8) if width_mult > 1.0 else 1792\n",
    "        self.conv = conv_1x1_bn(input_channel, output_channel)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Linear(output_channel, num_classes)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.weight.data.normal_(0, 0.001)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "def effnetv2_s(**kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a EfficientNetV2-S model\n",
    "    \"\"\"\n",
    "    cfgs = [\n",
    "        # t（）, c（通道数）, n(重复数), s（stride）, SE\n",
    "        [1,  24,  2, 1, 0],\n",
    "        [4,  48,  4, 2, 0],\n",
    "        [4,  64,  4, 2, 0],\n",
    "        [4, 128,  6, 2, 1],\n",
    "        [6, 160,  9, 1, 1],\n",
    "        [6, 256, 15, 2, 1],\n",
    "    ]\n",
    "    return EffNetV2(cfgs, **kwargs)\n",
    "\n",
    "\n",
    "def effnetv2_m(**kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a EfficientNetV2-M model\n",
    "    \"\"\"\n",
    "    cfgs = [\n",
    "        # t, c, n, s, SE\n",
    "        [1,  24,  3, 1, 0],\n",
    "        [4,  48,  5, 2, 0],\n",
    "        [4,  80,  5, 2, 0],\n",
    "        [4, 160,  7, 2, 1],\n",
    "        [6, 176, 14, 1, 1],\n",
    "        [6, 304, 18, 2, 1],\n",
    "        [6, 512,  5, 1, 1],\n",
    "    ]\n",
    "    return EffNetV2(cfgs, **kwargs)\n",
    "\n",
    "\n",
    "def effnetv2_l(**kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a EfficientNetV2-L model\n",
    "    \"\"\"\n",
    "    cfgs = [\n",
    "        # t, c, n, s, SE\n",
    "        [1,  32,  4, 1, 0],\n",
    "        [4,  64,  7, 2, 0],\n",
    "        [4,  96,  7, 2, 0],\n",
    "        [4, 192, 10, 2, 1],\n",
    "        [6, 224, 19, 1, 1],\n",
    "        [6, 384, 25, 2, 1],\n",
    "        [6, 640,  7, 1, 1],\n",
    "    ]\n",
    "    return EffNetV2(cfgs, **kwargs)\n",
    "\n",
    "\n",
    "def effnetv2_xl(**kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a EfficientNetV2-XL model\n",
    "    \"\"\"\n",
    "    cfgs = [\n",
    "        # t, c, n, s, SE\n",
    "        [1,  32,  4, 1, 0],\n",
    "        [4,  64,  8, 2, 0],\n",
    "        [4,  96,  8, 2, 0],\n",
    "        [4, 192, 16, 2, 1],\n",
    "        [6, 256, 24, 1, 1],\n",
    "        [6, 512, 32, 2, 1],\n",
    "        [6, 640,  8, 1, 1],\n",
    "    ]\n",
    "    return EffNetV2(cfgs, **kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
