{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "legitimate-happening",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "from transformer import Block\n",
    "from SSE import Block_Se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "strong-consequence",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetD(nn.Module):\n",
    "    \n",
    "    def __init__(self,opt):\n",
    "        super(NetD, self).__init__()\n",
    "        base_channels=32\n",
    "        self.encoder1 = Encoder_(add_final_conv=False)\n",
    "        self.final_conv=nn.Sequential(nn.Conv2d(base_channels*16, 1, 4, 1, 0, bias=False),\n",
    "                                nn.Sigmoid())\n",
    "            \n",
    "    def forward(self, input):\n",
    "        output=self.encoder1(input)\n",
    "        y=self.final_conv(output)\n",
    "        classifier = y.view(-1, 1).squeeze(1)\n",
    "        return classifier,output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "exposed-germany",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=NetD()\n",
    "input=torch.rand((1, 3, 128, 128))\n",
    "classifier,output=model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "certain-saturn",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1]), torch.Size([1, 512, 4, 4]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.shape,output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-thickness",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "frozen-freeze",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetG(nn.Module):\n",
    "    \"\"\"\n",
    "    GENERATOR NETWORK\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(NetG, self).__init__()\n",
    "#         opt.imageSize, opt.nz, opt.nc, opt.ngf, opt.ngpu, opt.n_extra_layers\n",
    "        self.encoder1 = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        self.encoder2 = Encoder_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent_i,y= self.encoder1(x)\n",
    "        \n",
    "        gen_imag = self.decoder(latent_i)\n",
    "        latent_o = self.encoder2(gen_imag)\n",
    "        \n",
    "#         print('gen_imag, latent_i, latent_o',gen_imag.shape, latent_i.shape, latent_o.shape)\n",
    "        return gen_imag, y, latent_o\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "interior-imagination",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=NetG()\n",
    "input=torch.rand((1, 3, 128, 128))\n",
    "out1,out2,out3=model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "binary-helping",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 128, 128]),\n",
       " torch.Size([1, 100, 1, 1]),\n",
       " torch.Size([1, 100, 1, 1]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1.shape,out2.shape,out3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tight-meaning",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "incredible-catch",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    DCGAN DECODER NETWORK\n",
    "    \"\"\"\n",
    "    def __init__(self, imageSize=128, nz=100, nc=3, ngf=64, ngpu=1, n_extra_layers=0):\n",
    "                \n",
    "        super(Decoder, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        assert imageSize % 16 == 0, \"imageSize has to be a multiple of 16\"\n",
    "\n",
    "        cngf, timageSize = ngf // 2, 4\n",
    "        while timageSize != imageSize:\n",
    "            cngf = cngf * 2\n",
    "            timageSize = timageSize * 2\n",
    "        base_channels=32\n",
    "        \n",
    "        self.pyramid0 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(base_channels*16,base_channels*8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(base_channels*8),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.pyramid1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(base_channels*8,base_channels*4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(base_channels*4),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.pyramid2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(base_channels*4,base_channels*2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(base_channels*2),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.pyramid3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(base_channels*2,base_channels, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(base_channels),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "#         self.pyramid4 = nn.Sequential(\n",
    "#             nn.ConvTranspose2d(64,32, 4, 2, 1, bias=False),\n",
    "#             nn.BatchNorm2d(32),\n",
    "#             nn.ReLU(True),\n",
    "#         )\n",
    "        self.final0 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(base_channels,nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "#         input=self.pyramid0_(input)\n",
    "        \n",
    "        input=self.pyramid0(input)\n",
    "        input=self.pyramid1(input)\n",
    "        input=self.pyramid2(input)\n",
    "        input=self.pyramid3(input)\n",
    "#         input=self.pyramid4(input)\n",
    "        input=self.final0(input)\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "attended-assembly",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Decoder()\n",
    "input=torch.rand((1, 512, 4, 4))\n",
    "out=model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "attempted-newfoundland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 128, 128])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "sticky-compromise",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    DCGAN ENCODER NETWORK\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, imageSize=128, nz=100, nc=3, ngf=64, ngpu=1, n_extra_layers=0, add_final_conv=True):\n",
    "       \n",
    "        super(Encoder, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        assert imageSize % 16 == 0, \"imageSize has to be a multiple of 16\"\n",
    "        \n",
    "        base_channels=32\n",
    "        \n",
    "        self.initial0 = nn.Sequential(#1,3,128,128->1,64,64,64\n",
    "            nn.Conv2d(nc, base_channels, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(base_channels),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.pyramid0 = nn.Sequential(#1,64,64,64->1,128,32,32\n",
    "            nn.Conv2d(base_channels, base_channels*2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(base_channels*2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        stream0 = nn.ModuleList()\n",
    "        stream0.append(Block_Se(in_channels=base_channels*2, out_channels=base_channels*2))#1,128,32,32->1,128,32,32\n",
    "        stream0.append(Block_Se(in_channels=base_channels*2, out_channels=base_channels*2))#1,128,32,32->1,128,32,32\n",
    "#         stream0.append(Block_Se(in_channels=base_channels*2, out_channels=base_channels*2))#1,128,32,32->1,128,32,32\n",
    "        self.stream0=nn.Sequential(*stream0)\n",
    "        self.down0 = Block_Se(in_channels=base_channels*2, out_channels=base_channels*4,kernel_size=3,stride=2)#1,128,32,32->1,256,16,16\n",
    "        \n",
    "        \n",
    "        self.pyramid1 = nn.Sequential(#1,128,32,32->1,256,16,16\n",
    "            nn.Conv2d(base_channels*2, base_channels*4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(base_channels*4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        stream1 = nn.ModuleList()\n",
    "        stream1.append(Block_Se(in_channels=base_channels*4, out_channels=base_channels*4))#1,256,16,16->1,256,16,16\n",
    "        stream1.append(Block_Se(in_channels=base_channels*4, out_channels=base_channels*4))#1,256,16,16->1,256,16,16\n",
    "        self.stream1=nn.Sequential(*stream1)\n",
    "        self.down1 = Block_Se(in_channels=base_channels*8, out_channels=base_channels*8,kernel_size=3,stride=2)#1,512,16,16->1,512,8,8\n",
    "        \n",
    "        self.pyramid2 = nn.Sequential(#1,256,16,16->1,512,8,8\n",
    "            nn.Conv2d(base_channels*4, base_channels*8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(base_channels*8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        stream2 = nn.ModuleList()\n",
    "        stream2.append(Block_Se(in_channels=base_channels*8, out_channels=base_channels*8))#1,512,8,8->1,512,8,8\n",
    "        stream2.append(Block_Se(in_channels=base_channels*8, out_channels=base_channels*8))#1,512,8,8->1,512,8,8\n",
    "        self.stream2=nn.Sequential(*stream2)\n",
    "        self.down2 = Block_Se(in_channels=base_channels*16, out_channels=base_channels*16,kernel_size=3,stride=2)#1,1024,8,8->1,1024,4,4\n",
    "        \n",
    "        self.pyramid3 = nn.Sequential(#1,512,8,8->1,1024,4,4\n",
    "            nn.Conv2d(base_channels*8, base_channels*16, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(base_channels*16),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "#         self.down3 = Block_Se(in_channels=base_channels*32, out_channels=base_channels*16,kernel_size=1,stride=1)#1,2048,4,4->1,1024,4,4\n",
    "        \n",
    "        self.down3 = nn.Sequential(\n",
    "            nn.Conv2d(base_channels*32, base_channels*16, 1, 1,bias=False),\n",
    "            nn.BatchNorm2d(base_channels*16),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        \n",
    "        if add_final_conv:\n",
    "            self.final_conv=nn.Conv2d(base_channels*16, nz, 4, 1, 0, bias=False)\n",
    "            \n",
    "    def forward(self, input):\n",
    "        output=self.initial0(input)\n",
    "        \n",
    "        output=self.pyramid0(output)\n",
    "        output0=self.stream0(output)\n",
    "        output0=self.down0(output)\n",
    "        \n",
    "        output=self.pyramid1(output)\n",
    "        output1=self.stream1(output)\n",
    "        output_01 = torch.cat([output0, output1], 1)\n",
    "        output1=self.down1(output_01)\n",
    "        \n",
    "        output=self.pyramid2(output)\n",
    "        output2=self.stream2(output)\n",
    "        output_12 = torch.cat([output1, output2], 1)\n",
    "        output2=self.down2(output_12)\n",
    "        \n",
    "#         print(output.shape)\n",
    "        output=self.pyramid3(output)\n",
    "        output_23 = torch.cat([output2, output], 1)\n",
    "        output2=self.down3(output_23)\n",
    "#         x.append(output)\n",
    "        y=self.final_conv(output)    \n",
    "        return output2,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "informational-flower",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Encoder()\n",
    "input=torch.rand((1,3,128,128))\n",
    "out,y=model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "governing-drove",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 512, 4, 4]), torch.Size([1, 100, 1, 1]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "suffering-injury",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_(nn.Module):\n",
    "    \"\"\"\n",
    "    DCGAN ENCODER NETWORK\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, imageSize=128, nz=100, nc=3, ngf=64, ngpu=1, n_extra_layers=0, add_final_conv=True):\n",
    "       \n",
    "        super(Encoder_, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        assert imageSize % 16 == 0, \"imageSize has to be a multiple of 16\"\n",
    "        \n",
    "        base_channels=32\n",
    "        \n",
    "        self.initial0 = nn.Sequential(#1,3,128,128->1,64,64,64\n",
    "            nn.Conv2d(nc, base_channels, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(base_channels),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.pyramid0 = nn.Sequential(#1,64,64,64->1,128,32,32\n",
    "            nn.Conv2d(base_channels, base_channels*2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(base_channels*2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        \n",
    "        self.pyramid1 = nn.Sequential(#1,128,32,32->1,256,16,16\n",
    "            nn.Conv2d(base_channels*2, base_channels*4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(base_channels*4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "       \n",
    "        self.pyramid2 = nn.Sequential(#1,256,16,16->1,512,8,8\n",
    "            nn.Conv2d(base_channels*4, base_channels*8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(base_channels*8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "       \n",
    "        self.pyramid3 = nn.Sequential(#1,512,8,8->1,1024,4,4\n",
    "            nn.Conv2d(base_channels*8, base_channels*16, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(base_channels*16),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "     \n",
    "        if add_final_conv:\n",
    "            self.final_conv=nn.Conv2d(base_channels*16, nz, 4, 1, 0, bias=False)\n",
    "        self.add_final_conv=add_final_conv  \n",
    "    def forward(self, input):\n",
    "        output=self.initial0(input)\n",
    "        \n",
    "        output=self.pyramid0(output)\n",
    "        \n",
    "        output=self.pyramid1(output)\n",
    "        \n",
    "        output=self.pyramid2(output)\n",
    "        \n",
    "#         print(output.shape)\n",
    "        output=self.pyramid3(output)\n",
    "        if self.add_final_conv:\n",
    "            y=self.final_conv(output)   \n",
    "            return y\n",
    "        else:\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "golden-adams",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Encoder_()\n",
    "input=torch.rand((1,3,128,128))\n",
    "out=model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "facial-crack",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 1, 1])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quarterly-supervision",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
