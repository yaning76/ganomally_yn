{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "indian-great",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "combined-lawrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "class h_sigmoid(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(h_sigmoid, self).__init__()\n",
    "        self.relu = nn.ReLU6(inplace=inplace)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(x + 3) / 6\n",
    "\n",
    "class h_swish(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(h_swish, self).__init__()\n",
    "        self.sigmoid = h_sigmoid(inplace=inplace)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.sigmoid(x)\n",
    "\n",
    "class CoordAtt(nn.Module):\n",
    "    def __init__(self, inp, oup, reduction=32):\n",
    "        super(CoordAtt, self).__init__()\n",
    "        self.pool_h = nn.AdaptiveAvgPool2d((None, 1))\n",
    "        self.pool_w = nn.AdaptiveAvgPool2d((1, None))\n",
    "\n",
    "        mip = max(8, inp // reduction)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(inp, mip, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(mip)\n",
    "        self.act = h_swish()\n",
    "        \n",
    "        self.conv_h = nn.Conv2d(mip, oup, kernel_size=1, stride=1, padding=0)\n",
    "        self.conv_w = nn.Conv2d(mip, oup, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        n,c,h,w = x.size()\n",
    "        x_h = self.pool_h(x)\n",
    "        x_w = self.pool_w(x).permute(0, 1, 3, 2)\n",
    "\n",
    "        y = torch.cat([x_h, x_w], dim=2)\n",
    "        y = self.conv1(y)\n",
    "        y = self.bn1(y)\n",
    "        y = self.act(y) \n",
    "        \n",
    "        x_h, x_w = torch.split(y, [h, w], dim=2)\n",
    "        x_w = x_w.permute(0, 1, 3, 2)\n",
    "\n",
    "        a_h = self.conv_h(x_h).sigmoid()\n",
    "        a_w = self.conv_w(x_w).sigmoid()\n",
    "\n",
    "        out = identity * a_w * a_h\n",
    "#         print(out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "grateful-routine",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=CoordAtt(256,256)\n",
    "x1=torch.rand((32,256,16,16))\n",
    "x=model(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "narrative-staff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 256, 16, 16])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "distinguished-alpha",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseConv(nn.Module):\n",
    "    \"\"\"A Conv2d -> Batchnorm -> silu/leaky relu block\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, ksize, stride, groups=1, bias=False, act=\"silu\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # same padding\n",
    "        pad = (ksize - 1) // 2\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=ksize,\n",
    "            stride=stride,\n",
    "            padding=pad,\n",
    "            groups=groups,\n",
    "            bias=bias,\n",
    "        )\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "    def fuseforward(self, x):\n",
    "        return self.act(self.conv(x))\n",
    "    \n",
    "class test(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(test, self).__init__()\n",
    "        base_channels=64\n",
    "        self.conv1=BaseConv(base_channels*4, base_channels*16, 3, 4)\n",
    "        self.conv2=BaseConv(base_channels*8, base_channels*16, 3, 2)\n",
    "        self.ca1=CoordAtt(base_channels*4,base_channels*4)\n",
    "        self.ca2=CoordAtt(base_channels*8,base_channels*8)\n",
    "        self.conv=BaseConv(base_channels*16, base_channels*16, 3, 1)\n",
    "    def forward(self, x1,x2,x3):\n",
    "        print(x1.shape)\n",
    "        x1=self.ca1(x1)\n",
    "        print(x1.shape)\n",
    "        x1=self.conv1(x1)\n",
    "        print(x1.shape)\n",
    "        x2=self.ca2(x2)\n",
    "        x2=self.conv2(x2)\n",
    "        \n",
    "        x=x1+x2+x3\n",
    "        x=self.conv(x)\n",
    "        print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cooperative-better",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 256, 16, 16])\n",
      "torch.Size([32, 256, 16, 16])\n",
      "torch.Size([32, 1024, 4, 4])\n",
      "torch.Size([32, 1024, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "model=test()\n",
    "x1=torch.rand((32,256,16,16))\n",
    "x2=torch.rand((32,512,8,8))\n",
    "x3=torch.rand((32,1024,4,4))\n",
    "x=model(x1,x2,x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "labeled-fisher",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1024, 4, 4])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "choice-airport",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseConv(nn.Module):\n",
    "    \"\"\"A Conv2d -> Batchnorm -> silu/leaky relu block\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, ksize, stride, groups=1, bias=False, act=\"silu\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # same padding\n",
    "        pad = (ksize - 1) // 2\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=ksize,\n",
    "            stride=stride,\n",
    "            padding=pad,\n",
    "            groups=groups,\n",
    "            bias=bias,\n",
    "        )\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "    def fuseforward(self, x):\n",
    "        return self.act(self.conv(x))\n",
    "    \n",
    "class test(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(test, self).__init__()\n",
    "        base_channels=64\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode=\"nearest\")\n",
    "        self.conv3=BaseConv(base_channels*16, base_channels*8, 3, 1)\n",
    "        self.conv2=BaseConv(base_channels*8, base_channels*4, 3, 1)\n",
    "        \n",
    "        self.conv1=BaseConv(base_channels*4, base_channels*8, 3, 2)\n",
    "        self.conv2_=BaseConv(base_channels*8, base_channels*16, 3, 2)\n",
    "    def forward(self, x1,x2,x3):\n",
    "        x3=self.upsample(x3)\n",
    "        x3=self.conv3(x3)\n",
    "        x2=x3+x2\n",
    "        x2=self.upsample(x2)\n",
    "        x2=self.conv2(x2)\n",
    "        x1=x2+x1\n",
    "        \n",
    "        y2=self.conv1(x1)\n",
    "        y2=y2+\n",
    "        y2=self.conv2_(x2)\n",
    "        return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "controversial-relation",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=test()\n",
    "x1=torch.rand((32,256,16,16))\n",
    "x2=torch.rand((32,512,8,8))\n",
    "x3=torch.rand((32,1024,4,4))\n",
    "x=model(x1,x2,x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "common-shopper",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 256, 16, 16])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "danish-settle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-correspondence",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-senate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checked-berkeley",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "angry-accommodation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spp_size(h,w, out_pool_size):\n",
    "    h_wid = math.ceil(h / out_pool_size)\n",
    "    w_wid = math.ceil(w / out_pool_size)\n",
    "    h_str = math.floor(h / out_pool_size)\n",
    "    w_str = math.floor(w / out_pool_size)\n",
    "    return h_wid,w_wid,h_str,w_str\n",
    "class BaseConv(nn.Module):\n",
    "    \"\"\"A Conv2d -> Batchnorm -> silu/leaky relu block\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, ksize, stride, groups=1, bias=False, act=\"silu\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # same padding\n",
    "        pad = (ksize - 1) // 2\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=ksize,\n",
    "            stride=stride,\n",
    "            padding=pad,\n",
    "            groups=groups,\n",
    "            bias=bias,\n",
    "        )\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "    def fuseforward(self, x):\n",
    "        return self.act(self.conv(x))\n",
    "    \n",
    "class test(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(test, self).__init__()\n",
    "        base_channels=64\n",
    "        self.conv1=BaseConv(base_channels*4, base_channels*16, ksize=1, stride=1)\n",
    "        self.pool1=nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        self.conv2=BaseConv(base_channels*8, base_channels*16, ksize=1, stride=1)\n",
    "        self.pool2=nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        self.pool3=nn.AdaptiveAvgPool2d(1)\n",
    "        self.bn = nn.BatchNorm1d(1)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        b=1\n",
    "        gamma=2\n",
    "        channel=base_channels*16\n",
    "        kernel_size = int(abs((math.log(channel, 2) + b) / gamma))\n",
    "        kernel_size = kernel_size if kernel_size % 2 else kernel_size + 1\n",
    "        print(kernel_size)\n",
    "        self.conv = nn.Conv1d(1, 1, kernel_size=kernel_size, padding=(kernel_size - 1) // 2, bias=False)\n",
    "    def forward(self, x1,x2,x3):\n",
    "        b,_,_,_=x1.size()\n",
    "        x1=self.conv1(x1)\n",
    "        x1=self.pool1(x1)\n",
    "        \n",
    "        print(x1.shape)\n",
    "        \n",
    "        x2=self.conv2(x2)\n",
    "        x2=self.pool2(x2)\n",
    "        print(x2.shape)\n",
    "        \n",
    "        x3=self.pool3(x3)\n",
    "        print(x3.shape)\n",
    "        x=x1+x2+x3\n",
    "        \n",
    "#         x=torch.cat((x1,x2,x3), 1).unsqueeze(1)\n",
    "#         x=self.conv(x).transpose(-1, -2).unsqueeze(-1)\n",
    "#         x=self.conv(x)\n",
    "        print(x.shape)\n",
    "        x=x.squeeze(-1).transpose(-1, -2)\n",
    "        x=self.bn(x)\n",
    "        x=self.act(x).transpose(-1, -2).unsqueeze(-1)\n",
    "        print(x.shape)\n",
    "        return x1,x2,x3,x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "better-recipient",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "torch.Size([32, 1024, 1, 1])\n",
      "torch.Size([32, 1024, 1, 1])\n",
      "torch.Size([32, 1024, 1, 1])\n",
      "torch.Size([32, 1024, 1, 1])\n",
      "torch.Size([32, 1024, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "model=test()\n",
    "x1=torch.rand((32,256,16,16))\n",
    "x2=torch.rand((32,512,8,8))\n",
    "x3=torch.rand((32,1024,4,4))\n",
    "x1,x2,x3,x=model(x1,x2,x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usual-output",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "economic-ethernet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spp_size(h,w, out_pool_size):\n",
    "    h_wid = math.ceil(h / out_pool_size)\n",
    "    w_wid = math.ceil(w / out_pool_size)\n",
    "    h_str = math.floor(h / out_pool_size)\n",
    "    w_str = math.floor(w / out_pool_size)\n",
    "    return h_wid,w_wid,h_str,w_str\n",
    "class BaseConv(nn.Module):\n",
    "    \"\"\"A Conv2d -> Batchnorm -> silu/leaky relu block\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, ksize, stride, groups=1, bias=False, act=\"silu\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # same padding\n",
    "        pad = (ksize - 1) // 2\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=ksize,\n",
    "            stride=stride,\n",
    "            padding=pad,\n",
    "            groups=groups,\n",
    "            bias=bias,\n",
    "        )\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "    def fuseforward(self, x):\n",
    "        return self.act(self.conv(x))\n",
    "    \n",
    "class test(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(test, self).__init__()\n",
    "        base_channels=64\n",
    "        self.conv1=BaseConv(base_channels*4, base_channels*16, ksize=1, stride=1)\n",
    "        self.pool1=nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        self.conv2=BaseConv(base_channels*8, base_channels*16, ksize=1, stride=1)\n",
    "        self.pool2=nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        self.pool3=nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        b=1\n",
    "        gamma=2\n",
    "        channel=base_channels*16\n",
    "        kernel_size = int(abs((math.log(channel, 2) + b) / gamma))\n",
    "        kernel_size = kernel_size if kernel_size % 2 else kernel_size + 1\n",
    "        print(kernel_size)\n",
    "        self.conv = nn.Conv1d(1, 1, kernel_size=kernel_size, padding=(kernel_size - 1) // 2, bias=False)\n",
    "    def forward(self, x1,x2,x3):\n",
    "        b,_,_,_=x1.size()\n",
    "        \n",
    "        x1=self.pool1(x1)\n",
    "        x1=self.conv1(x1)\n",
    "        print(x1.shape)\n",
    "        \n",
    "        x2=self.conv2(x2)\n",
    "        x2=self.pool2(x2).view(b, -1)\n",
    "        \n",
    "        x3=self.pool3(x3).view(b, -1)\n",
    "        x=torch.cat((x1,x2,x3), 1).unsqueeze(1)\n",
    "#         x=self.conv(x).transpose(-1, -2).unsqueeze(-1)\n",
    "#         x=self.conv(x)\n",
    "        \n",
    "        return x1,x2,x3,x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "false-header",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "torch.Size([32, 1024, 1, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 4 and 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-1ceaef37a60d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx3\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/YN/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-2f529a55b0a6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x1, x2, x3)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mx3\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;31m#         x=self.conv(x).transpose(-1, -2).unsqueeze(-1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m#         x=self.conv(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 4 and 2"
     ]
    }
   ],
   "source": [
    "model=test()\n",
    "x1=torch.rand((32,256,16,16))\n",
    "x2=torch.rand((32,512,8,8))\n",
    "x3=torch.rand((32,1024,4,4))\n",
    "x1,x2,x3,x=model(x1,x2,x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silent-logan",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "academic-reputation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-letter",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subject-employment",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "modular-longer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spp_size(h,w, out_pool_size):\n",
    "    h_wid = math.ceil(h / out_pool_size)\n",
    "    w_wid = math.ceil(w / out_pool_size)\n",
    "    h_str = math.floor(h / out_pool_size)\n",
    "    w_str = math.floor(w / out_pool_size)\n",
    "    return h_wid,w_wid,h_str,w_str\n",
    "class BaseConv(nn.Module):\n",
    "    \"\"\"A Conv2d -> Batchnorm -> silu/leaky relu block\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, ksize, stride, groups=1, bias=False, act=\"silu\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # same padding\n",
    "        pad = (ksize - 1) // 2\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=ksize,\n",
    "            stride=stride,\n",
    "            padding=pad,\n",
    "            groups=groups,\n",
    "            bias=bias,\n",
    "        )\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "    def fuseforward(self, x):\n",
    "        return self.act(self.conv(x))\n",
    "    \n",
    "class test(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(test, self).__init__()\n",
    "        base_channels=64\n",
    "        self.conv1=BaseConv(base_channels*4, base_channels*16, ksize=1, stride=1)\n",
    "        size1=get_spp_size(16,16,1)\n",
    "        self.pool1=nn.MaxPool2d(kernel_size=(size1[0], size1[1]), stride=(size1[2], size1[3]))\n",
    "        \n",
    "        self.conv2=BaseConv(base_channels*8, base_channels*16, ksize=1, stride=1)\n",
    "        size1=get_spp_size(8,8,2)\n",
    "        self.pool2=nn.MaxPool2d(kernel_size=(size1[0], size1[1]), stride=(size1[2], size1[3]))\n",
    "        \n",
    "        size1=get_spp_size(4,4,4)\n",
    "        self.pool3=nn.MaxPool2d(kernel_size=(size1[0], size1[1]), stride=(size1[2], size1[3]))\n",
    "        b=1\n",
    "        gamma=2\n",
    "        channel=base_channels*16\n",
    "        kernel_size = int(abs((math.log(channel, 2) + b) / gamma))\n",
    "        kernel_size = kernel_size if kernel_size % 2 else kernel_size + 1\n",
    "        print(kernel_size)\n",
    "#         self.conv = nn.Conv1d(1, 1, kernel_size=kernel_size, padding=(kernel_size - 1) // 2, bias=False)\n",
    "        self.conv=nn.MaxPool1d(kernel_size=4)\n",
    "    def forward(self, x1,x2,x3):\n",
    "        b,_,_,_=x1.size()\n",
    "        x1=self.conv1(x1)\n",
    "        x1=self.pool1(x1).view(b, -1)\n",
    "        \n",
    "        x2=self.conv2(x2)\n",
    "        x2=self.pool2(x2).view(b, -1)\n",
    "        \n",
    "        x3=self.pool3(x3).view(b, -1)\n",
    "        x=torch.cat((x1,x2,x3), 1).unsqueeze(1)\n",
    "#         x=self.conv(x).transpose(-1, -2).unsqueeze(-1)\n",
    "#         x=self.conv(x)\n",
    "        \n",
    "        return x1,x2,x3,x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "gorgeous-satin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "model=test()\n",
    "x1=torch.rand((32,256,16,16))\n",
    "x2=torch.rand((32,512,8,8))\n",
    "x3=torch.rand((32,1024,4,4))\n",
    "x1,x2,x3,x=model(x1,x2,x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "spatial-production",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1024]),\n",
       " torch.Size([32, 4096]),\n",
       " torch.Size([32, 16384]),\n",
       " torch.Size([32, 1, 21504]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.shape,x2.shape,x3.shape,x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "tough-bacon",
   "metadata": {},
   "outputs": [],
   "source": [
    "class eca_block(nn.Module):\n",
    "    def __init__(self, channel, b=1, gamma=2):\n",
    "        super(eca_block, self).__init__()\n",
    "        kernel_size = int(abs((math.log(channel, 2) + b) / gamma))\n",
    "        print(kernel_size)\n",
    "        kernel_size = kernel_size if kernel_size % 2 else kernel_size + 1\n",
    "        print(kernel_size)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.conv = nn.Conv1d(1, 1, kernel_size=kernel_size, padding=(kernel_size - 1) // 2, bias=False) \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        y = self.avg_pool(x)\n",
    "        print(y.shape)\n",
    "        z=y.squeeze(-1).transpose(-1, -2)\n",
    "        print(z.shape)\n",
    "        z=self.conv(y.squeeze(-1).transpose(-1, -2))\n",
    "        print(z.shape)\n",
    "        y = self.conv(y.squeeze(-1).transpose(-1, -2)).transpose(-1, -2).unsqueeze(-1)\n",
    "        print(y.shape)\n",
    "        y = self.sigmoid(y)\n",
    "        return x * y.expand_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "peaceful-classification",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "5\n",
      "torch.Size([1, 256, 64, 64])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 1, 256])\n",
      "torch.Size([1, 1, 256])\n",
      "torch.Size([1, 256, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "model=eca_block(256)\n",
    "input=torch.rand((1,256,64,64))\n",
    "out=model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "completed-growing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bibliographic-hands",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv1d(1, 1, kernel_size=kernel_size, padding=(kernel_size - 1) // 2, bias=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-uzbekistan",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-raise",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "later-thanks",
   "metadata": {},
   "outputs": [],
   "source": [
    "input=torch.rand((32, 86016))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "existing-blowing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 86016, 1, 1])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.view(32,86016,1,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "august-crime",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 86016])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "related-stupid",
   "metadata": {},
   "outputs": [],
   "source": [
    "input0=torch.rand((32,256,16,16))\n",
    "input1=torch.rand((32,256,8,8))\n",
    "input2=torch.rand((32,256,4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "floppy-therapy",
   "metadata": {},
   "outputs": [],
   "source": [
    "b=32\n",
    "input0=input0.view(b, -1)\n",
    "input1=input1.view(b, -1)\n",
    "input2=input2.view(b, -1)\n",
    "x=torch.cat((input0, input1,input2), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "quiet-village",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 4096])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "intended-classroom",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 86016])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "blocked-force",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spp_size(h,w, out_pool_size):\n",
    "    h_wid = math.ceil(h / out_pool_size)\n",
    "    w_wid = math.ceil(w / out_pool_size)\n",
    "    h_str = math.floor(h / out_pool_size)\n",
    "    w_str = math.floor(w / out_pool_size)\n",
    "    return h_wid,w_wid,h_str,w_str\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "intimate-operation",
   "metadata": {},
   "outputs": [],
   "source": [
    "t=get_spp_size(8,8,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "powerful-offer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dying-myrtle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_pyramid_pool(previous_conv, num_sample, previous_conv_size, out_pool_size):\n",
    "    \"\"\"\n",
    "    previous_conv: a tensor vector of previous convolution layer\n",
    "    num_sample: an int number of image in the batch\n",
    "    previous_conv_size: an int vector [height, width] of the matrix features size of previous convolution layer\n",
    "    out_pool_size: a int vector of expected output size of max pooling layer\n",
    "    returns: a tensor vector with shape [1 x n] is the concentration of multi-level pooling\n",
    "\"\"\"\n",
    "    for i in range(len(out_pool_size)):\n",
    "        # out_pool_size是一个数组，例如[1,2,4],表示需要将原先的特征图分别划分为1×1、2×2、4×4三种.\n",
    "        h, w = previous_conv_size\n",
    "        # h,w表示原先特征图的长和宽\n",
    "        h_wid = math.ceil(h / out_pool_size[i])\n",
    "        w_wid = math.ceil(w / out_pool_size[i])\n",
    "        # 计算每一个子块的长和宽，这里需要进行取整\n",
    "        h_str = math.floor(h / out_pool_size[i])\n",
    "        w_str = math.floor(w / out_pool_size[i])\n",
    "        # 计算池化的步长\n",
    "        max_pool = nn.MaxPool2d(kernel_size=(h_wid, w_wid), stride=(h_str, w_str))\n",
    "        x = max_pool(previous_conv)\n",
    "        print(x.shape)\n",
    "        print(x.view(num_sample, -1).shape)\n",
    "        # 对每个子块进行最大池化\n",
    "        if i == 0:\n",
    "            spp = x.view(num_sample, -1)\n",
    "#             print(spp.shape)\n",
    "        else:\n",
    "            spp = torch.cat((spp, x.view(num_sample, -1)), 1)\n",
    "#             print(spp.shape)\n",
    "     # 拼接各个子块的输出结果\n",
    "    return spp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "rubber-collect",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 256, 1, 1])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 256, 2, 2])\n",
      "torch.Size([32, 1024])\n",
      "torch.Size([32, 256, 4, 4])\n",
      "torch.Size([32, 4096])\n"
     ]
    }
   ],
   "source": [
    "input=torch.rand((32,256,64,64))\n",
    "spp=spatial_pyramid_pool(input, 32, [64,64], [1,2,4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "entitled-gamma",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 5376])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "trained-lambda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseConv(nn.Module):\n",
    "    \"\"\"A Conv2d -> Batchnorm -> silu/leaky relu block\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, ksize, stride, groups=1, bias=False, act=\"silu\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # same padding\n",
    "        pad = (ksize - 1) // 2\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=ksize,\n",
    "            stride=stride,\n",
    "            padding=pad,\n",
    "            groups=groups,\n",
    "            bias=bias,\n",
    "        )\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.act = get_activation(act, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "    def fuseforward(self, x):\n",
    "        return self.act(self.conv(x))\n",
    "def get_activation(name=\"silu\", inplace=True):\n",
    "    if name == \"silu\":\n",
    "        module = nn.SiLU(inplace=inplace)\n",
    "    elif name == \"relu\":\n",
    "        module = nn.ReLU(inplace=inplace)\n",
    "    elif name == \"lrelu\":\n",
    "        module = nn.LeakyReLU(0.1, inplace=inplace)\n",
    "    else:\n",
    "        raise AttributeError(\"Unsupported act type: {}\".format(name))\n",
    "    return module\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eligible-burst",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPPBottleneck(nn.Module):\n",
    "    \"\"\"Spatial pyramid pooling layer used in YOLOv3-SPP\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, kernel_sizes=(5, 9, 13), activation=\"silu\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        hidden_channels = in_channels // 2\n",
    "        self.conv1 = BaseConv(in_channels, hidden_channels, 1, stride=1, act=activation)\n",
    "        self.m = nn.ModuleList(\n",
    "            [\n",
    "                nn.MaxPool2d(kernel_size=ks, stride=1, padding=ks // 2)\n",
    "                for ks in kernel_sizes\n",
    "            ]\n",
    "        )\n",
    "        conv2_channels = hidden_channels * (len(kernel_sizes) + 1)\n",
    "        self.conv2 = BaseConv(conv2_channels, out_channels, 1, stride=1, act=activation)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        print(x.shape)\n",
    "        for m in self.m:\n",
    "            t=m(x)\n",
    "            print(t.shape)\n",
    "        x = torch.cat([x] + [m(x) for m in self.m], dim=1)\n",
    "        x = self.conv2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "taken-stick",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 64, 64])\n",
      "torch.Size([1, 128, 64, 64])\n",
      "torch.Size([1, 128, 64, 64])\n",
      "torch.Size([1, 128, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "model=SPPBottleneck(in_channels=256, out_channels=64)\n",
    "input=torch.rand((1,256,64,64))\n",
    "out=model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acceptable-mount",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 64, 64])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-threat",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confident-blond",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "spread-cologne",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAM_Attention(nn.Module):  \n",
    "    def __init__(self, in_channels, out_channels, rate=4):  \n",
    "        super(GAM_Attention, self).__init__()  \n",
    "\n",
    "        self.channel_attention = nn.Sequential(  \n",
    "            nn.Linear(in_channels, int(in_channels / rate)),  \n",
    "            nn.ReLU(inplace=True),  \n",
    "            nn.Linear(int(in_channels / rate), in_channels)  \n",
    "        )  \n",
    "      \n",
    "    def forward(self, x):  \n",
    "        b, c, h, w = x.shape  \n",
    "        x_permute = x.permute(0, 2, 3, 1).view(b, -1, c)  \n",
    "        x_att_permute = self.channel_attention(x_permute).view(b, h, w, c) \n",
    "        print(x_att_permute.shape)\n",
    "        x_channel_att = x_att_permute.permute(0, 3, 1, 2)  \n",
    "      \n",
    "        x = x * x_channel_att  \n",
    "        return x_channel_att,x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "filled-green",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 48, 64])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':  \n",
    "    x = torch.randn(1, 64, 32, 48)  \n",
    "    b, c, h, w = x.shape  \n",
    "    net = GAM_Attention(in_channels=c, out_channels=c)  \n",
    "    y,y2 = net(x)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "least-investment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 64, 32, 48]), torch.Size([1, 64, 32, 48]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " y.shape,y2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-twelve",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn  \n",
    "import torch  \n",
    "\n",
    "\n",
    "class GAM_Attention(nn.Module):  \n",
    "    def __init__(self, in_channels, out_channels, rate=4):  \n",
    "        super(GAM_Attention, self).__init__()  \n",
    "\n",
    "        self.channel_attention = nn.Sequential(  \n",
    "            nn.Linear(in_channels, int(in_channels / rate)),  \n",
    "            nn.ReLU(inplace=True),  \n",
    "            nn.Linear(int(in_channels / rate), in_channels)  \n",
    "        )  \n",
    "      \n",
    "        self.spatial_attention = nn.Sequential(  \n",
    "            nn.Conv2d(in_channels, int(in_channels / rate), kernel_size=7, padding=3),  \n",
    "            nn.BatchNorm2d(int(in_channels / rate)),  \n",
    "            nn.ReLU(inplace=True),  \n",
    "            nn.Conv2d(int(in_channels / rate), out_channels, kernel_size=7, padding=3),  \n",
    "            nn.BatchNorm2d(out_channels)  \n",
    "        )  \n",
    "      \n",
    "    def forward(self, x):  \n",
    "        b, c, h, w = x.shape  \n",
    "        x_permute = x.permute(0, 2, 3, 1).view(b, -1, c)  \n",
    "        x_att_permute = self.channel_attention(x_permute).view(b, h, w, c)  \n",
    "        x_channel_att = x_att_permute.permute(0, 3, 1, 2)  \n",
    "      \n",
    "        x = x * x_channel_att  \n",
    "      \n",
    "        x_spatial_att = self.spatial_attention(x).sigmoid()  \n",
    "        out = x * x_spatial_att  \n",
    "      \n",
    "        return out  \n",
    "\n",
    "if __name__ == '__main__':  \n",
    "    x = torch.randn(1, 64, 32, 48)  \n",
    "    b, c, h, w = x.shape  \n",
    "    net = GAM_Attention(in_channels=c, out_channels=c)  \n",
    "    y = net(x)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-perfume",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-chemical",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "african-communication",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unnecessary-bangkok",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.conv_r = ConvBlock(in_channels=256, out_channels=64, kernel_size=1, stride=1, padding=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "planned-estimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=2, padding=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                               bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.conv1(input)\n",
    "        return self.relu(self.bn(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "chief-citizen",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ConvBlock(in_channels=256, out_channels=64, kernel_size=1, stride=1, padding=0)\n",
    "input=torch.rand((1,256,64,64))\n",
    "out=model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "funny-omaha",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 64, 64])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape#1, 64, 128, 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acting-scout",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_r4 = self.conv_r(x_r4)\n",
    "x_r3 = self.up_r1(x_r4, x_r3)#3:1,128,128,128->1,64,128,128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "academic-teddy",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCA_(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.in_channels = in_channels\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "\n",
    "    def forward(self, input):\n",
    "        # global average pooling\n",
    "        x = self.avgpool(input)\n",
    "        assert self.in_channels == x.size(1), 'in_channels and out_channels should all be {}'.format(x.size(1))\n",
    "        x = self.conv(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = torch.mul(input, x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "european-tuesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCA(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.in_channels = in_channels\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "\n",
    "    def forward(self, input):\n",
    "        # global average pooling\n",
    "        x = self.avgpool(input)\n",
    "        assert self.in_channels == x.size(1), 'in_channels and out_channels should all be {}'.format(x.size(1))\n",
    "        x = self.conv(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = torch.mul(input, x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bound-vitamin",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=FCA(64,64)\n",
    "input=torch.rand((1,64,64,64))\n",
    "out=model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cooperative-height",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 64, 64])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varying-patrol",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownsamplingBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size,\n",
    "                 stride=2, padding=1, dilation=1, groups=1, avg_pool=True,\n",
    "                 se_block=True, activation=nn.ReLU(),\n",
    "                 ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.groups = groups\n",
    "        self.stride = stride\n",
    "        self.kernel_size = kernel_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.padding = padding\n",
    "        self.se_block = se_block\n",
    "\n",
    "        assert padding == 1\n",
    "        padding_11 = padding - 3 // 2\n",
    "\n",
    "        self.fused = False\n",
    "\n",
    "        self.dense_groups = groups\n",
    "        self.nonlinearity = activation\n",
    "        \n",
    "        self.rbr_dense = conv_bn(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            groups=1) if (kernel_size != 1) else None\n",
    "        \n",
    "        self.rbr_1x1 = nn.Sequential(\n",
    "                nn.AvgPool2d(2, 2),\n",
    "                conv_bn(in_channels=in_channels, out_channels=out_channels,\n",
    "                        kernel_size=1, stride=1, padding=0, groups=groups))\n",
    "\n",
    "        if self.se_block:\n",
    "            self.se = SE1(\n",
    "                in_channels, out_channels, g=groups,\n",
    "                ver=2 if (out_channels != in_channels or stride != 1) else 1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        rbr_1x1_output=self.rbr_1x1(inputs)\n",
    "        drop_path_output = self.rbr_dense(inputs)\n",
    "        out = drop_path_output + rbr_1x1_output \n",
    "        out = out * self.se(inputs)############\n",
    "        out = self.nonlinearity(out)\n",
    "        return out\n",
    "    def fuse_conv_bn(self, conv, bn):\n",
    "\n",
    "        std = (bn.running_var + bn.eps).sqrt()\n",
    "        bias = bn.bias - bn.running_mean * bn.weight / std\n",
    "\n",
    "        t = (bn.weight / std).reshape(-1, 1, 1, 1)\n",
    "        weights = conv.weight * t\n",
    "\n",
    "        bn = nn.Identity()\n",
    "        conv = nn.Conv2d(in_channels=conv.in_channels,\n",
    "                         out_channels=conv.out_channels,\n",
    "                         kernel_size=conv.kernel_size,\n",
    "                         stride=conv.stride,\n",
    "                         padding=conv.padding,\n",
    "                         dilation=conv.dilation,\n",
    "                         groups=conv.groups,\n",
    "                         bias=True,\n",
    "                         padding_mode=conv.padding_mode)\n",
    "\n",
    "        conv.weight = torch.nn.Parameter(weights)\n",
    "        conv.bias = torch.nn.Parameter(bias)\n",
    "\n",
    "        return conv\n",
    "\n",
    "    def fuse_repvgg_block(self):\n",
    "        self.rbr_dense = self.fuse_conv_bn(self.rbr_dense.conv, self.rbr_dense.bn)\n",
    "\n",
    "        if isinstance(self.rbr_1x1, nn.Sequential) and isinstance(self.rbr_1x1[0], nn.AvgPool2d):\n",
    "            self.rbr_1x1[1] = self.fuse_conv_bn(self.rbr_1x1[1].conv, self.rbr_1x1[1].bn)\n",
    "            rbr_1x1_bias = self.rbr_1x1[1].bias\n",
    "\n",
    "            weight_1x1_expanded = torch.nn.functional.interpolate(self.rbr_1x1[1].weight, scale_factor=2.0, mode='nearest')\n",
    "            weight_1x1_expanded = weight_1x1_expanded / 4\n",
    "            weight_1x1_expanded = torch.nn.functional.pad(weight_1x1_expanded, [1, 0, 1, 0])\n",
    "        else:\n",
    "            self.rbr_1x1 = self.fuse_conv_bn(self.rbr_1x1.conv, self.rbr_1x1.bn)\n",
    "            rbr_1x1_bias = self.rbr_1x1.bias\n",
    "\n",
    "            weight_1x1_expanded = torch.nn.functional.pad(self.rbr_1x1.weight, [1, 1, 1, 1])\n",
    "\n",
    "        self.rbr_dense.weight = torch.nn.Parameter(self.rbr_dense.weight + weight_1x1_expanded)\n",
    "        self.rbr_dense.bias = torch.nn.Parameter(self.rbr_dense.bias + rbr_1x1_bias)\n",
    "\n",
    "        self.rbr_1x1 = nn.Identity()\n",
    "\n",
    "        self.fused = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-arabic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-terrorism",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-candidate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "proved-philip",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autopad(k, p=None):  # kernel, padding\n",
    "    # Pad to 'same'\n",
    "    if p is None:\n",
    "        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]  # auto-pad\n",
    "#         print(p)\n",
    "    return p\n",
    "class SE1(nn.Module):\n",
    "    # Squeeze-and-excitation block in https://arxiv.org/abs/1709.01507\n",
    "    # ch_in, ch_out, number, shortcut, groups, expansion\n",
    "    def __init__(self, c_in, c_out, n=1, shortcut=True,  g=1, e=0.5, ver=1):\n",
    "        super(SE1, self).__init__()\n",
    "        self.ver = ver\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.cvsig = ConvSig(c_in, c_out, 1, 1, g=g)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cvsig(self.avg_pool(x))\n",
    "        if self.ver == 2:\n",
    "            x = 2 * x\n",
    "        return x\n",
    "def conv_bn(in_channels, out_channels, kernel_size, stride, padding, groups=1):\n",
    "    result = nn.Sequential()\n",
    "    result.add_module(\n",
    "        'conv',\n",
    "        nn.Conv2d(\n",
    "            in_channels=in_channels, out_channels=out_channels,\n",
    "            kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "            groups=groups, bias=False))\n",
    "    result.add_module('bn', nn.BatchNorm2d(num_features=out_channels))\n",
    "    return result\n",
    "class ConvSig(nn.Module):\n",
    "    # Standard convolution\n",
    "    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):  # ch_in, ch_out, kernel, stride, padding, groups\n",
    "        super(ConvSig, self).__init__()\n",
    "        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g, bias=False)\n",
    "        self.act = nn.Sigmoid() if act else nn.Identity()\n",
    "#         print(autopad(k, p))\n",
    "    def forward(self, x):\n",
    "        \n",
    "        return self.act(self.conv(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-yukon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-stupid",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exterior-cooperative",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "acknowledged-atlanta",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "model=Encoder(imageSize=128, nz=100, nc=3, ngf=64, ngpu=1, n_extra_layers=0, add_final_conv=True)\n",
    "input=torch.rand((1,3,128,128))\n",
    "out,y=model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "logical-service",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1024, 4, 4]), torch.Size([1, 100, 1, 1]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-evolution",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abroad-lyric",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaning-luther",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designing-fishing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quarterly-evans",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minor-election",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-highlight",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-crack",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepVGGBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size,\n",
    "                 stride=1, padding=0, dilation=1, groups=1,\n",
    "                 padding_mode='zeros', avg_pool=True,\n",
    "                 se_block=True, activation=nn.SiLU(),\n",
    "                 ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.groups = groups\n",
    "        self.stride = stride\n",
    "        self.kernel_size = kernel_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.padding = padding\n",
    "        self.padding_mode = padding_mode\n",
    "        self.se_block = se_block\n",
    "\n",
    "        assert padding == 1\n",
    "        padding_11 = padding - 3 // 2\n",
    "\n",
    "        self.fused = False\n",
    "\n",
    "        self.dense_groups = groups\n",
    "        self.nonlinearity = activation\n",
    "\n",
    "        self.rbr_identity = nn.BatchNorm2d(\n",
    "            num_features=in_channels) if (\n",
    "                out_channels == in_channels and\n",
    "                stride == 1) else None\n",
    "        self.rbr_dense = conv_bn(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            groups=self.dense_groups) if (kernel_size != 1) else None\n",
    "        self.rbr_1x1 = conv_bn(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=1,\n",
    "            stride=stride,\n",
    "            padding=padding_11,\n",
    "            groups=groups)\n",
    "        if stride == 2 and avg_pool:\n",
    "            self.rbr_1x1 = nn.Sequential(\n",
    "                nn.AvgPool2d(2, 2),\n",
    "                conv_bn(in_channels=in_channels, out_channels=out_channels,\n",
    "                        kernel_size=1, stride=1, padding=0, groups=groups)\n",
    "            )\n",
    "\n",
    "        # updated to reuse code\n",
    "        self.channel_shuffle = (groups > 1)\n",
    "\n",
    "        if self.se_block:\n",
    "            self.se = SE1(\n",
    "                in_channels, out_channels, g=groups,\n",
    "                ver=2 if (out_channels != in_channels or stride != 1) else 1)\n",
    "\n",
    "    def _forward(self, inputs):\n",
    "        if not self.fused:#1*1 conv\n",
    "            rbr_1x1_output = self.rbr_1x1(inputs)\n",
    "        else:\n",
    "            rbr_1x1_output = None\n",
    "\n",
    "        if self.rbr_dense is None:#3*3 conv\n",
    "            dense_output = 0\n",
    "        else:\n",
    "            dense_output = self.rbr_dense(inputs)\n",
    "\n",
    "        return rbr_1x1_output, dense_output\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if self.rbr_identity is None:\n",
    "            id_out = 0\n",
    "        else:\n",
    "            id_out = self.rbr_identity(inputs)#BN\n",
    "\n",
    "        rbr_1x1_output, drop_path_output = self._forward(inputs)\n",
    "\n",
    "        if self.se_block:\n",
    "            if self.rbr_identity is not None:\n",
    "                id_out = id_out * self.se(id_out)###########\n",
    "\n",
    "        if not self.fused:\n",
    "            out = drop_path_output + rbr_1x1_output + id_out\n",
    "        else:\n",
    "            out = drop_path_output + id_out\n",
    "\n",
    "        if self.se_block and (self.rbr_identity is None):\n",
    "            out = out * self.se(inputs)############\n",
    "\n",
    "        out = self.nonlinearity(out)\n",
    "\n",
    "        if self.channel_shuffle:\n",
    "            out = channel_shuffle(out, self.groups)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def fuse_conv_bn(self, conv, bn):\n",
    "        \"\"\"\n",
    "        # n,c,h,w - conv\n",
    "        # n - bn (scale, bias, mean, var)\n",
    "\n",
    "        if type(bn) is nn.Identity or type(bn) is None:\n",
    "            return\n",
    "\n",
    "        conv.weight\n",
    "        running_mean = bn.running_mean\n",
    "        running_var = bn.running_var\n",
    "        gamma = bn.weight\n",
    "        beta = bn.bias\n",
    "        eps = bn.eps\n",
    "        \"\"\"\n",
    "        std = (bn.running_var + bn.eps).sqrt()\n",
    "        bias = bn.bias - bn.running_mean * bn.weight / std\n",
    "\n",
    "        t = (bn.weight / std).reshape(-1, 1, 1, 1)\n",
    "        weights = conv.weight * t\n",
    "\n",
    "        bn = nn.Identity()\n",
    "        conv = nn.Conv2d(in_channels=conv.in_channels,\n",
    "                         out_channels=conv.out_channels,\n",
    "                         kernel_size=conv.kernel_size,\n",
    "                         stride=conv.stride,\n",
    "                         padding=conv.padding,\n",
    "                         dilation=conv.dilation,\n",
    "                         groups=conv.groups,\n",
    "                         bias=True,\n",
    "                         padding_mode=conv.padding_mode)\n",
    "\n",
    "        conv.weight = torch.nn.Parameter(weights)\n",
    "        conv.bias = torch.nn.Parameter(bias)\n",
    "\n",
    "        return conv\n",
    "\n",
    "    def fuse_repvgg_block(self):\n",
    "        self.rbr_dense = self.fuse_conv_bn(self.rbr_dense.conv, self.rbr_dense.bn)\n",
    "\n",
    "        if isinstance(self.rbr_1x1, nn.Sequential) and isinstance(self.rbr_1x1[0], nn.AvgPool2d):\n",
    "            self.rbr_1x1[1] = self.fuse_conv_bn(self.rbr_1x1[1].conv, self.rbr_1x1[1].bn)\n",
    "            rbr_1x1_bias = self.rbr_1x1[1].bias\n",
    "\n",
    "            weight_1x1_expanded = torch.nn.functional.interpolate(self.rbr_1x1[1].weight, scale_factor=2.0, mode='nearest')\n",
    "            weight_1x1_expanded = weight_1x1_expanded / 4\n",
    "            weight_1x1_expanded = torch.nn.functional.pad(weight_1x1_expanded, [1, 0, 1, 0])\n",
    "        else:\n",
    "            self.rbr_1x1 = self.fuse_conv_bn(self.rbr_1x1.conv, self.rbr_1x1.bn)\n",
    "            rbr_1x1_bias = self.rbr_1x1.bias\n",
    "\n",
    "            weight_1x1_expanded = torch.nn.functional.pad(self.rbr_1x1.weight, [1, 1, 1, 1])\n",
    "\n",
    "        self.rbr_dense.weight = torch.nn.Parameter(self.rbr_dense.weight + weight_1x1_expanded)\n",
    "        self.rbr_dense.bias = torch.nn.Parameter(self.rbr_dense.bias + rbr_1x1_bias)\n",
    "\n",
    "        self.rbr_1x1 = nn.Identity()\n",
    "\n",
    "        self.fused = True\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
