{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "transsexual-carter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torchstat import stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "presidential-washer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class se_block(nn.Module):\n",
    "    def __init__(self, channel, ratio=16):\n",
    "        super(se_block, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "                nn.Linear(channel, channel // ratio, bias=False),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(channel // ratio, channel, bias=False),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "expired-median",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=se_block(channel=64, ratio=16)\n",
    "input=torch.rand((1,64,128,128))\n",
    "x=model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "virgin-brave",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 128, 128])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "determined-component",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forbidden-lesson",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-theater",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adolescent-legislation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "    \n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        x = self.conv(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    DCGAN DECODER NETWORK\n",
    "    \"\"\"\n",
    "    def __init__(self, imageSize, nz, nc, ngf, ngpu, n_extra_layers=0):\n",
    "        # nz : dimensionality of the latent space潜在空间的维度\n",
    "        # nc : number of image channels\n",
    "        # ndf : channels of middle layers for generator 生成器中间层通道数\n",
    "        # ngpu : number of gpu\n",
    "        # n_extra_layers : extra layers of Encoder and Decoder\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        assert imageSize % 16 == 0, \"imageSize has to be a multiple of 16\"\n",
    "\n",
    "        cngf, timageSize = ngf // 2, 4\n",
    "        while timageSize != imageSize:\n",
    "            cngf = cngf * 2\n",
    "            timageSize = timageSize * 2\n",
    "        \n",
    "        self.up1=Up(in_channels=2048, out_channels=1024, bilinear=True)\n",
    "        self.up2=Up(in_channels=1024, out_channels=512, bilinear=True)\n",
    "        self.up3=Up(in_channels=512, out_channels=256, bilinear=True)\n",
    "        self.up4=Up(in_channels=256, out_channels=128, bilinear=True)\n",
    "        self.up5=Up(in_channels=128, out_channels=64, bilinear=True)\n",
    "        self.final0 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64,nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh(),\n",
    "        ) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.up1(x)\n",
    "        x = self.up2(x)\n",
    "        x = self.up3(x)\n",
    "        x = self.up4(x)\n",
    "        x = self.up5(x)\n",
    "        x=self.final0(x)\n",
    "        \n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "conventional-serve",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    \"\"\"\n",
    "    This function is taken from the original tf repo.\n",
    "    It ensures that all layers have a channel number that is divisible by 8\n",
    "    It can be seen here:\n",
    "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    :param v:\n",
    "    :param divisor:\n",
    "    :param min_value:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "def conv_3x3_bn(inp, oup, stride):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        SiLU()\n",
    "    )\n",
    "if hasattr(nn, 'SiLU'):\n",
    "    SiLU = nn.SiLU\n",
    "else:\n",
    "    # For compatibility with old PyTorch versions\n",
    "    class SiLU(nn.Module):\n",
    "        def forward(self, x):\n",
    "            return x * torch.sigmoid(x)\n",
    "        \n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, inp, oup, reduction=4):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "                nn.Linear(oup, _make_divisible(inp // reduction, 8)),\n",
    "                SiLU(),\n",
    "                nn.Linear(_make_divisible(inp // reduction, 8), oup),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y\n",
    "        \n",
    "class MBConv(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, expand_ratio, use_se):\n",
    "        super(MBConv, self).__init__()\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        hidden_dim = round(inp * expand_ratio)\n",
    "        self.identity = stride == 1 and inp == oup\n",
    "        if use_se:\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw\n",
    "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                SiLU(),\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                SiLU(),\n",
    "                SELayer(inp, hidden_dim),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                # fused\n",
    "                nn.Conv2d(inp, hidden_dim, 3, stride, 1, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                SiLU(),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.identity:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "\n",
    "            return self.conv(x)\n",
    "        \n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    DCGAN ENCODER NETWORK\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, imageSize, nz, nc, ngf, ngpu, n_extra_layers=0, add_final_conv=True):\n",
    "        # nz : dimensionality of the latent space潜在空间的维度\n",
    "        # nc : number of image channels\n",
    "        # ndf : channels of middle layers for generator 生成器中间层通道数\n",
    "        # ngpu : number of gpu\n",
    "        # n_extra_layers : extra layers of Encoder and Decoder\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        assert imageSize % 16 == 0, \"imageSize has to be a multiple of 16\"\n",
    "        \n",
    "        self.conv1=conv_3x3_bn(inp=3, oup=64, stride=2)\n",
    "        self.Fused_MBConv_0=MBConv(inp=64, oup=64, stride=1, expand_ratio=1, use_se=0)\n",
    "        self.Fused_MBConv_1=MBConv(inp=64, oup=128, stride=2, expand_ratio=4, use_se=0)\n",
    "        self.Fused_MBConv_2=MBConv(inp=128, oup=256, stride=2, expand_ratio=4, use_se=0)\n",
    "        \n",
    "        self.MBConv_3=MBConv(inp=256, oup=512, stride=2, expand_ratio=4, use_se=1)\n",
    "        self.MBConv_4=MBConv(inp=512, oup=1024, stride=2, expand_ratio=6, use_se=1)\n",
    "        self.MBConv_5=MBConv(inp=1024, oup=2048, stride=2, expand_ratio=6, use_se=1)\n",
    "        self.final=nn.Conv2d(2048, nz, 4, 1, 0, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)#[1, 64, 64, 64]\n",
    "#         print(x.shape)\n",
    "        x = self.Fused_MBConv_0(x)#[1, 64, 64, 64]\n",
    "#         print(x.shape)\n",
    "        x = self.Fused_MBConv_1(x)#1, 128, 32, 32\n",
    "#         print(x.shape)\n",
    "        x=self.Fused_MBConv_2(x)#1, 256, 16, 16\n",
    "#         print(x.shape)\n",
    "        x = self.MBConv_3(x)#1, 256, 16, 16\n",
    "#         print(x.shape)\n",
    "        x = self.MBConv_4(x)\n",
    "        print(x.shape)\n",
    "        x = self.MBConv_5(x)#1, 512, 8, 8\n",
    "        print(x.shape)\n",
    "        y = self.final(x)\n",
    "        \n",
    "        return x,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "advisory-shelf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetG(nn.Module):\n",
    "    \"\"\"\n",
    "    GENERATOR NETWORK\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    def __init__(self,dim=2048, num_heads=4, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0.,\n",
    "                 attn_drop_rate=0., drop_path_rate=0.,norm_layer=nn.LayerNorm, linear=False,depths=5):\n",
    "        super(NetG, self).__init__()\n",
    "                \n",
    "        imageSize=256\n",
    "        nz=100\n",
    "        nc=3\n",
    "        ngf=64\n",
    "        ngpu=1\n",
    "        n_extra_layers=0\n",
    "            \n",
    "        self.depths = depths\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depths)]\n",
    "        \n",
    "        self.transformer_blocks = nn.ModuleList([Block(\n",
    "                dim=dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[j], norm_layer=norm_layer,\n",
    "                linear=linear)\n",
    "                for j in range(depths)])\n",
    "        \n",
    "        self.encoder1 = Encoder(imageSize, nz, nc, ngf, ngpu, n_extra_layers)\n",
    "        self.decoder = Decoder(imageSize, nz, nc, ngf, ngpu, n_extra_layers)\n",
    "        self.encoder2 = Encoder(imageSize, nz, nc, ngf, ngpu, n_extra_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent_i,y= self.encoder1(x)\n",
    "        \n",
    "        b,c,h,w=latent_i.shape\n",
    "        latent_i = latent_i.flatten(2).transpose(1, 2)\n",
    "        for i in range(len(self.transformer_blocks)):\n",
    "            latent_i=self.transformer_blocks[i](latent_i,h,w)\n",
    "        latent_i = latent_i.reshape(b, h, w, -1).permute(0, 3, 1, 2).contiguous()\n",
    "#         print(latent_i.shape)\n",
    "\n",
    "        gen_imag = self.decoder(latent_i)\n",
    "        latent_o,y1 = self.encoder2(gen_imag)\n",
    "        \n",
    "#         print('gen_imag, latent_i, latent_o',gen_imag.shape, latent_i.shape, latent_o.shape)\n",
    "        return gen_imag, y, y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "purple-weekly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024, 8, 8])\n",
      "torch.Size([1, 2048, 4, 4])\n",
      "torch.Size([1, 1024, 8, 8])\n",
      "torch.Size([1, 2048, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "model=NetG(dim=2048, num_heads=4, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0.,\n",
    "                 attn_drop_rate=0., drop_path_rate=0.,norm_layer=nn.LayerNorm, linear=False,depths=5)\n",
    "input=torch.rand((1,3,256,256))\n",
    "gen_imag, y, latent_o=model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "white-engineer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 256, 256])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_imag.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "honey-latter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 1, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "polished-trinity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 1, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fifty-windsor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "from timm.models.registry import register_model\n",
    "from timm.models.vision_transformer import _cfg\n",
    "import math\n",
    "\n",
    "class DWConv(nn.Module):\n",
    "    def __init__(self, dim=768):\n",
    "        super(DWConv, self).__init__()\n",
    "        self.dwconv = nn.Conv2d(dim, dim, 3, 1, 1, bias=True, groups=dim)\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        B, N, C = x.shape\n",
    "        x = x.transpose(1, 2).view(B, C, H, W)\n",
    "        x = self.dwconv(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0., linear=False):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.dwconv = DWConv(hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        self.linear = linear\n",
    "        if self.linear:\n",
    "            self.relu = nn.ReLU(inplace=True)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            fan_out //= m.groups\n",
    "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        x = self.fc1(x)\n",
    "        if self.linear:\n",
    "            x = self.relu(x)\n",
    "        x = self.dwconv(x, H, W)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., sr_ratio=1, linear=False):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, f\"dim {dim} should be divided by num_heads {num_heads}.\"\n",
    "\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.q = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.kv = nn.Linear(dim, dim * 2, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        self.linear = linear\n",
    "        self.sr_ratio = sr_ratio\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            fan_out //= m.groups\n",
    "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        B, N, C = x.shape\n",
    "        q = self.q(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        kv = self.kv(x).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        k, v = kv[0], kv[1]\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "    \n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, linear=False):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        \n",
    "        self.attn = Attention(\n",
    "            dim,\n",
    "            num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "            attn_drop=attn_drop, proj_drop=drop, linear=linear)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop, linear=linear)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        self.norm = nn.LayerNorm(1024)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            fan_out //= m.groups\n",
    "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self,x,H,W):\n",
    " #         B, C, H, W = x.shape\n",
    "\n",
    "#         x = x.flatten(2).transpose(1, 2)\n",
    "#         x = self.norm(x)\n",
    "        \n",
    "        x = x + self.drop_path(self.attn(self.norm1(x), H, W))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x), H, W))\n",
    "#         print(x.shape)\n",
    "#         x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
    "#         print(x.shape)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italic-middle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-tomorrow",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-fifty",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustained-variable",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recovered-curve",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-light",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-master",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "supreme-welding",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "    \n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        x = self.conv(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adjusted-belgium",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Up(in_channels=512, out_channels=256, bilinear=True)\n",
    "input=torch.rand((1,512,64,64))\n",
    "x=model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "controlled-gibson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 128, 128])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aerial-identifier",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "    \n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        x = self.conv(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    DCGAN DECODER NETWORK\n",
    "    \"\"\"\n",
    "    def __init__(self, imageSize, nz, nc, ngf, ngpu, n_extra_layers=0):\n",
    "        # nz : dimensionality of the latent space潜在空间的维度\n",
    "        # nc : number of image channels\n",
    "        # ndf : channels of middle layers for generator 生成器中间层通道数\n",
    "        # ngpu : number of gpu\n",
    "        # n_extra_layers : extra layers of Encoder and Decoder\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        assert imageSize % 16 == 0, \"imageSize has to be a multiple of 16\"\n",
    "\n",
    "        cngf, timageSize = ngf // 2, 4\n",
    "        while timageSize != imageSize:\n",
    "            cngf = cngf * 2\n",
    "            timageSize = timageSize * 2\n",
    "        \n",
    "        self.up1=Up(in_channels=2048, out_channels=1024, bilinear=True)\n",
    "        self.up2=Up(in_channels=1024, out_channels=512, bilinear=True)\n",
    "        self.up3=Up(in_channels=512, out_channels=256, bilinear=True)\n",
    "        self.up4=Up(in_channels=256, out_channels=128, bilinear=True)\n",
    "        self.up5=Up(in_channels=128, out_channels=64, bilinear=True)\n",
    "        self.final0 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64,nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh(),\n",
    "        ) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.up1(x)\n",
    "        x = self.up2(x)\n",
    "        x = self.up3(x)\n",
    "        x = self.up4(x)\n",
    "        x = self.up5(x)\n",
    "        x=self.final0(x)\n",
    "        \n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "divine-symphony",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MAdd]: Upsample is not supported!\n",
      "[Memory]: Upsample is not supported!\n",
      "[MAdd]: Upsample is not supported!\n",
      "[Memory]: Upsample is not supported!\n",
      "[MAdd]: Upsample is not supported!\n",
      "[Memory]: Upsample is not supported!\n",
      "[MAdd]: Upsample is not supported!\n",
      "[Memory]: Upsample is not supported!\n",
      "[MAdd]: Upsample is not supported!\n",
      "[Memory]: Upsample is not supported!\n",
      "[Flops]: ConvTranspose2d is not supported!\n",
      "[Memory]: ConvTranspose2d is not supported!\n",
      "[MAdd]: Tanh is not supported!\n",
      "[Flops]: Tanh is not supported!\n",
      "[Memory]: Tanh is not supported!\n"
     ]
    }
   ],
   "source": [
    "model=Decoder(imageSize=256, nz=100, nc=3, ngf=64, ngpu=1, n_extra_layers=0)\n",
    "input=torch.rand((1,2048, 4, 4))\n",
    "x=model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "acceptable-deadline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MAdd]: Upsample is not supported!\n",
      "[Memory]: Upsample is not supported!\n",
      "[MAdd]: Upsample is not supported!\n",
      "[Memory]: Upsample is not supported!\n",
      "[MAdd]: Upsample is not supported!\n",
      "[Memory]: Upsample is not supported!\n",
      "[MAdd]: Upsample is not supported!\n",
      "[Memory]: Upsample is not supported!\n",
      "[MAdd]: Upsample is not supported!\n",
      "[Memory]: Upsample is not supported!\n",
      "[Flops]: ConvTranspose2d is not supported!\n",
      "[Memory]: ConvTranspose2d is not supported!\n",
      "[MAdd]: Tanh is not supported!\n",
      "[Flops]: Tanh is not supported!\n",
      "[Memory]: Tanh is not supported!\n",
      "                  module name   input shape  output shape      params memory(MB)              MAdd            Flops  MemRead(B)  MemWrite(B) duration[%]    MemR+W(B)\n",
      "0                      up1.up  2048   4   4  2048   8   8         0.0       0.50               0.0             64.0         0.0          0.0       7.70%          0.0\n",
      "1      up1.conv.double_conv.0  2048   8   8  1024   8   8  18874368.0       0.25   2,415,853,568.0  1,207,959,552.0  76021760.0     262144.0      28.58%   76283904.0\n",
      "2      up1.conv.double_conv.1  1024   8   8  1024   8   8      2048.0       0.25         262,144.0        131,072.0    270336.0     262144.0       0.31%     532480.0\n",
      "3      up1.conv.double_conv.2  1024   8   8  1024   8   8         0.0       0.25          65,536.0         65,536.0    262144.0     262144.0       0.17%     524288.0\n",
      "4      up1.conv.double_conv.3  1024   8   8  1024   8   8   9437184.0       0.25   1,207,894,016.0    603,979,776.0  38010880.0     262144.0       9.45%   38273024.0\n",
      "5      up1.conv.double_conv.4  1024   8   8  1024   8   8      2048.0       0.25         262,144.0        131,072.0    270336.0     262144.0       0.29%     532480.0\n",
      "6      up1.conv.double_conv.5  1024   8   8  1024   8   8         0.0       0.25          65,536.0         65,536.0    262144.0     262144.0       0.16%     524288.0\n",
      "7                      up2.up  1024   8   8  1024  16  16         0.0       1.00               0.0            256.0         0.0          0.0       3.16%          0.0\n",
      "8      up2.conv.double_conv.0  1024  16  16   512  16  16   4718592.0       0.50   2,415,788,032.0  1,207,959,552.0  19922944.0     524288.0       7.03%   20447232.0\n",
      "9      up2.conv.double_conv.1   512  16  16   512  16  16      1024.0       0.50         524,288.0        262,144.0    528384.0     524288.0       0.31%    1052672.0\n",
      "10     up2.conv.double_conv.2   512  16  16   512  16  16         0.0       0.50         131,072.0        131,072.0    524288.0     524288.0       2.35%    1048576.0\n",
      "11     up2.conv.double_conv.3   512  16  16   512  16  16   2359296.0       0.50   1,207,828,480.0    603,979,776.0   9961472.0     524288.0       5.41%   10485760.0\n",
      "12     up2.conv.double_conv.4   512  16  16   512  16  16      1024.0       0.50         524,288.0        262,144.0    528384.0     524288.0       0.33%    1052672.0\n",
      "13     up2.conv.double_conv.5   512  16  16   512  16  16         0.0       0.50         131,072.0        131,072.0    524288.0     524288.0       0.16%    1048576.0\n",
      "14                     up3.up   512  16  16   512  32  32         0.0       2.00               0.0          1,024.0         0.0          0.0       0.68%          0.0\n",
      "15     up3.conv.double_conv.0   512  32  32   256  32  32   1179648.0       1.00   2,415,656,960.0  1,207,959,552.0   6815744.0    1048576.0       8.99%    7864320.0\n",
      "16     up3.conv.double_conv.1   256  32  32   256  32  32       512.0       1.00       1,048,576.0        524,288.0   1050624.0    1048576.0       0.38%    2099200.0\n",
      "17     up3.conv.double_conv.2   256  32  32   256  32  32         0.0       1.00         262,144.0        262,144.0   1048576.0    1048576.0       0.16%    2097152.0\n",
      "18     up3.conv.double_conv.3   256  32  32   256  32  32    589824.0       1.00   1,207,697,408.0    603,979,776.0   3407872.0    1048576.0       3.97%    4456448.0\n",
      "19     up3.conv.double_conv.4   256  32  32   256  32  32       512.0       1.00       1,048,576.0        524,288.0   1050624.0    1048576.0       0.42%    2099200.0\n",
      "20     up3.conv.double_conv.5   256  32  32   256  32  32         0.0       1.00         262,144.0        262,144.0   1048576.0    1048576.0       0.16%    2097152.0\n",
      "21                     up4.up   256  32  32   256  64  64         0.0       4.00               0.0          4,096.0         0.0          0.0       0.73%          0.0\n",
      "22     up4.conv.double_conv.0   256  64  64   128  64  64    294912.0       2.00   2,415,394,816.0  1,207,959,552.0   5373952.0    2097152.0       3.32%    7471104.0\n",
      "23     up4.conv.double_conv.1   128  64  64   128  64  64       256.0       2.00       2,097,152.0      1,048,576.0   2098176.0    2097152.0       0.46%    4195328.0\n",
      "24     up4.conv.double_conv.2   128  64  64   128  64  64         0.0       2.00         524,288.0        524,288.0   2097152.0    2097152.0       0.16%    4194304.0\n",
      "25     up4.conv.double_conv.3   128  64  64   128  64  64    147456.0       2.00   1,207,435,264.0    603,979,776.0   2686976.0    2097152.0       2.92%    4784128.0\n",
      "26     up4.conv.double_conv.4   128  64  64   128  64  64       256.0       2.00       2,097,152.0      1,048,576.0   2098176.0    2097152.0       0.45%    4195328.0\n",
      "27     up4.conv.double_conv.5   128  64  64   128  64  64         0.0       2.00         524,288.0        524,288.0   2097152.0    2097152.0       0.16%    4194304.0\n",
      "28                     up5.up   128  64  64   128 128 128         0.0       8.00               0.0         16,384.0         0.0          0.0       2.27%          0.0\n",
      "29     up5.conv.double_conv.0   128 128 128    64 128 128     73728.0       4.00   2,414,870,528.0  1,207,959,552.0   8683520.0    4194304.0       3.35%   12877824.0\n",
      "30     up5.conv.double_conv.1    64 128 128    64 128 128       128.0       4.00       4,194,304.0      2,097,152.0   4194816.0    4194304.0       1.21%    8389120.0\n",
      "31     up5.conv.double_conv.2    64 128 128    64 128 128         0.0       4.00       1,048,576.0      1,048,576.0   4194304.0    4194304.0       0.20%    8388608.0\n",
      "32     up5.conv.double_conv.3    64 128 128    64 128 128     36864.0       4.00   1,206,910,976.0    603,979,776.0   4341760.0    4194304.0       1.86%    8536064.0\n",
      "33     up5.conv.double_conv.4    64 128 128    64 128 128       128.0       4.00       4,194,304.0      2,097,152.0   4194816.0    4194304.0       0.69%    8389120.0\n",
      "34     up5.conv.double_conv.5    64 128 128    64 128 128         0.0       4.00       1,048,576.0      1,048,576.0   4194304.0    4194304.0       0.18%    8388608.0\n",
      "35                   final0.0    64 128 128     3 256 256      3072.0       0.75     100,614,144.0              0.0         0.0          0.0       1.54%          0.0\n",
      "36                   final0.1     3 256 256     3 256 256         0.0       0.75               0.0              0.0         0.0          0.0       0.32%          0.0\n",
      "total                                                      37722880.0      63.50  18,236,260,352.0  9,071,908,160.0         0.0          0.0     100.00%  256523264.0\n",
      "=====================================================================================================================================================================\n",
      "Total params: 37,722,880\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Total memory: 63.50MB\n",
      "Total MAdd: 18.24GMAdd\n",
      "Total Flops: 9.07GFlops\n",
      "Total MemR+W: 244.64MB\n",
      "\n",
      "Number of parameters: 37.72M\n"
     ]
    }
   ],
   "source": [
    "stat(model, (2048, 4, 4))\n",
    "total = sum([param.nelement() for param in model.parameters()])\n",
    "print(\"Number of parameters: %.2fM\" % (total/1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "million-distance",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    DCGAN DECODER NETWORK\n",
    "    \"\"\"\n",
    "    def __init__(self, imageSize, nz, nc, ngf, ngpu, n_extra_layers=0):\n",
    "        \n",
    "        # nz : dimensionality of the latent space潜在空间的维度\n",
    "        # nc : number of image channels\n",
    "        # ndf : channels of middle layers for generator 生成器中间层通道数\n",
    "        # ngpu : number of gpu\n",
    "        # n_extra_layers : extra layers of Encoder and Decoder\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        assert imageSize % 16 == 0, \"imageSize has to be a multiple of 16\"\n",
    "\n",
    "        cngf, timageSize = ngf // 2, 4\n",
    "        while timageSize != imageSize:\n",
    "            cngf = cngf * 2\n",
    "            timageSize = timageSize * 2\n",
    "        self.pyramid0_ = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2048,1024, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.pyramid0 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1024,512, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.pyramid1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512,256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.pyramid2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256,128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.pyramid3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128,64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.final0 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64,nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "        \n",
    "#         self.double_conv0 = nn.Sequential(\n",
    "#             nn.Conv2d(2048, 1024, kernel_size=3, padding=1, bias=False),\n",
    "#             nn.BatchNorm2d(1024),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(1024, 1024, kernel_size=3, padding=1, bias=False),\n",
    "#             nn.BatchNorm2d(1024),\n",
    "#             nn.ReLU(inplace=True)\n",
    "#         )\n",
    "#         self.double_conv1 = nn.Sequential(\n",
    "#             nn.Conv2d(1024, 512, kernel_size=3, padding=1, bias=False),\n",
    "#             nn.BatchNorm2d(512),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(512, 512, kernel_size=3, padding=1, bias=False),\n",
    "#             nn.BatchNorm2d(512),\n",
    "#             nn.ReLU(inplace=True)\n",
    "#         )\n",
    "#         self.double_conv2 = nn.Sequential(\n",
    "#             nn.Conv2d(512, 256, kernel_size=3, padding=1, bias=False),\n",
    "#             nn.BatchNorm2d(256),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(256, 256, kernel_size=3, padding=1, bias=False),\n",
    "#             nn.BatchNorm2d(256),\n",
    "#             nn.ReLU(inplace=True)\n",
    "#         )\n",
    "#         self.double_conv3 = nn.Sequential(\n",
    "#             nn.Conv2d(256, 128, kernel_size=3, padding=1, bias=False),\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(128, 128, kernel_size=3, padding=1, bias=False),\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.ReLU(inplace=True)\n",
    "#         )\n",
    "#         self.double_conv4 = nn.Sequential(\n",
    "#             nn.Conv2d(128, 64, kernel_size=3, padding=1, bias=False),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(64, 64, kernel_size=3, padding=1, bias=False),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU(inplace=True)\n",
    "#         )\n",
    "        \n",
    "        \n",
    "    def forward(self, input):\n",
    "        input=self.pyramid0_(input)\n",
    "        \n",
    "        input=self.pyramid0(input)\n",
    "        input=self.pyramid1(input)\n",
    "        input=self.pyramid2(input)\n",
    "        input=self.pyramid3(input)\n",
    "        \n",
    "        input=self.final0(input)\n",
    "        \n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "given-connectivity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Flops]: ConvTranspose2d is not supported!\n",
      "[Memory]: ConvTranspose2d is not supported!\n",
      "[Flops]: ConvTranspose2d is not supported!\n",
      "[Memory]: ConvTranspose2d is not supported!\n",
      "[Flops]: ConvTranspose2d is not supported!\n",
      "[Memory]: ConvTranspose2d is not supported!\n",
      "[Flops]: ConvTranspose2d is not supported!\n",
      "[Memory]: ConvTranspose2d is not supported!\n",
      "[Flops]: ConvTranspose2d is not supported!\n",
      "[Memory]: ConvTranspose2d is not supported!\n",
      "[Flops]: ConvTranspose2d is not supported!\n",
      "[Memory]: ConvTranspose2d is not supported!\n",
      "[Flops]: ConvTranspose2d is not supported!\n",
      "[Memory]: ConvTranspose2d is not supported!\n",
      "[Flops]: ConvTranspose2d is not supported!\n",
      "[Memory]: ConvTranspose2d is not supported!\n",
      "[Flops]: ConvTranspose2d is not supported!\n",
      "[Memory]: ConvTranspose2d is not supported!\n",
      "[Flops]: ConvTranspose2d is not supported!\n",
      "[Memory]: ConvTranspose2d is not supported!\n",
      "[Flops]: ConvTranspose2d is not supported!\n",
      "[Memory]: ConvTranspose2d is not supported!\n",
      "[Flops]: ConvTranspose2d is not supported!\n",
      "[Memory]: ConvTranspose2d is not supported!\n",
      "[Flops]: ConvTranspose2d is not supported!\n",
      "[Memory]: ConvTranspose2d is not supported!\n",
      "[Flops]: ConvTranspose2d is not supported!\n",
      "[Memory]: ConvTranspose2d is not supported!\n",
      "[Flops]: ConvTranspose2d is not supported!\n",
      "[Memory]: ConvTranspose2d is not supported!\n",
      "[Flops]: ConvTranspose2d is not supported!\n",
      "[Memory]: ConvTranspose2d is not supported!\n",
      "[Flops]: ConvTranspose2d is not supported!\n",
      "[Memory]: ConvTranspose2d is not supported!\n",
      "[Flops]: ConvTranspose2d is not supported!\n",
      "[Memory]: ConvTranspose2d is not supported!\n",
      "[Flops]: ConvTranspose2d is not supported!\n",
      "[Memory]: ConvTranspose2d is not supported!\n",
      "[Flops]: ConvTranspose2d is not supported!\n",
      "[Memory]: ConvTranspose2d is not supported!\n",
      "[Flops]: ConvTranspose2d is not supported!\n",
      "[Memory]: ConvTranspose2d is not supported!\n",
      "[Flops]: ConvTranspose2d is not supported!\n",
      "[Memory]: ConvTranspose2d is not supported!\n",
      "[Flops]: ConvTranspose2d is not supported!\n",
      "[Memory]: ConvTranspose2d is not supported!\n",
      "[Flops]: ConvTranspose2d is not supported!\n",
      "[Memory]: ConvTranspose2d is not supported!\n",
      "[Flops]: ConvTranspose2d is not supported!\n",
      "[Memory]: ConvTranspose2d is not supported!\n",
      "[Flops]: ConvTranspose2d is not supported!\n",
      "[Memory]: ConvTranspose2d is not supported!\n",
      "[Flops]: ConvTranspose2d is not supported!\n",
      "[Memory]: ConvTranspose2d is not supported!\n",
      "[Flops]: ConvTranspose2d is not supported!\n",
      "[Memory]: ConvTranspose2d is not supported!\n",
      "[Flops]: ConvTranspose2d is not supported!\n",
      "[Memory]: ConvTranspose2d is not supported!\n",
      "[Flops]: ConvTranspose2d is not supported!\n",
      "[Memory]: ConvTranspose2d is not supported!\n",
      "[MAdd]: Tanh is not supported!\n",
      "[Flops]: Tanh is not supported!\n",
      "[Memory]: Tanh is not supported!\n",
      "[MAdd]: Tanh is not supported!\n",
      "[Flops]: Tanh is not supported!\n",
      "[Memory]: Tanh is not supported!\n",
      "[MAdd]: Tanh is not supported!\n",
      "[Flops]: Tanh is not supported!\n",
      "[Memory]: Tanh is not supported!\n",
      "[MAdd]: Tanh is not supported!\n",
      "[Flops]: Tanh is not supported!\n",
      "[Memory]: Tanh is not supported!\n",
      "[MAdd]: Tanh is not supported!\n",
      "[Flops]: Tanh is not supported!\n",
      "[Memory]: Tanh is not supported!\n",
      "       module name   input shape  output shape      params memory(MB)             MAdd        Flops  MemRead(B)  MemWrite(B) duration[%]   MemR+W(B)\n",
      "0      pyramid0_.0  2048   4   4  1024   8   8  33554432.0       0.25  1,073,725,440.0          0.0         0.0          0.0      26.20%         0.0\n",
      "1      pyramid0_.1  1024   8   8  1024   8   8      2048.0       0.25        262,144.0    131,072.0    270336.0     262144.0       0.87%    532480.0\n",
      "2      pyramid0_.2  1024   8   8  1024   8   8         0.0       0.25         65,536.0     65,536.0    262144.0     262144.0       4.91%    524288.0\n",
      "3       pyramid0.0  1024   8   8   512  16  16   8388608.0       0.50  1,073,709,056.0          0.0         0.0          0.0      21.78%         0.0\n",
      "4       pyramid0.1   512  16  16   512  16  16      1024.0       0.50        524,288.0    262,144.0    528384.0     524288.0       0.82%   1052672.0\n",
      "5       pyramid0.2   512  16  16   512  16  16         0.0       0.50        131,072.0    131,072.0    524288.0     524288.0       0.44%   1048576.0\n",
      "6       pyramid1.0   512  16  16   256  32  32   2097152.0       1.00  1,073,676,288.0          0.0         0.0          0.0       9.62%         0.0\n",
      "7       pyramid1.1   256  32  32   256  32  32       512.0       1.00      1,048,576.0    524,288.0   1050624.0    1048576.0       1.00%   2099200.0\n",
      "8       pyramid1.2   256  32  32   256  32  32         0.0       1.00        262,144.0    262,144.0   1048576.0    1048576.0       0.46%   2097152.0\n",
      "9       pyramid2.0   256  32  32   128  64  64    524288.0       2.00  1,073,610,752.0          0.0         0.0          0.0      10.28%         0.0\n",
      "10      pyramid2.1   128  64  64   128  64  64       256.0       2.00      2,097,152.0  1,048,576.0   2098176.0    2097152.0       0.95%   4195328.0\n",
      "11      pyramid2.2   128  64  64   128  64  64         0.0       2.00        524,288.0    524,288.0   2097152.0    2097152.0       0.53%   4194304.0\n",
      "12      pyramid3.0   128  64  64    64 128 128    131072.0       4.00  1,073,479,680.0          0.0         0.0          0.0      16.31%         0.0\n",
      "13      pyramid3.1    64 128 128    64 128 128       128.0       4.00      4,194,304.0  2,097,152.0   4194816.0    4194304.0       1.28%   8389120.0\n",
      "14      pyramid3.2    64 128 128    64 128 128         0.0       4.00      1,048,576.0  1,048,576.0   4194304.0    4194304.0       0.56%   8388608.0\n",
      "15        final0.0    64 128 128     3 256 256      3072.0       0.75    100,614,144.0          0.0         0.0          0.0       3.15%         0.0\n",
      "16        final0.1     3 256 256     3 256 256         0.0       0.75              0.0          0.0         0.0          0.0       0.84%         0.0\n",
      "total                                           44702592.0      24.75  5,478,973,440.0  6,094,848.0         0.0          0.0     100.00%  32521728.0\n",
      "====================================================================================================================================================\n",
      "Total params: 44,702,592\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Total memory: 24.75MB\n",
      "Total MAdd: 5.48GMAdd\n",
      "Total Flops: 6.09MFlops\n",
      "Total MemR+W: 31.02MB\n",
      "\n",
      "Number of parameters: 44.70M\n"
     ]
    }
   ],
   "source": [
    "model=Decoder(imageSize=256, nz=100, nc=3, ngf=64, ngpu=1, n_extra_layers=0)\n",
    "input=torch.rand((1,2048, 4, 4))\n",
    "stat(model, (2048, 4, 4))\n",
    "total = sum([param.nelement() for param in model.parameters()])\n",
    "print(\"Number of parameters: %.2fM\" % (total/1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thorough-payroll",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-robertson",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realistic-comparative",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-governor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "included-causing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 256, 256])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "narrow-quest",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "equipped-field",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    \"\"\"\n",
    "    This function is taken from the original tf repo.\n",
    "    It ensures that all layers have a channel number that is divisible by 8\n",
    "    It can be seen here:\n",
    "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    :param v:\n",
    "    :param divisor:\n",
    "    :param min_value:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "def conv_3x3_bn(inp, oup, stride):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        SiLU()\n",
    "    )\n",
    "if hasattr(nn, 'SiLU'):\n",
    "    SiLU = nn.SiLU\n",
    "else:\n",
    "    # For compatibility with old PyTorch versions\n",
    "    class SiLU(nn.Module):\n",
    "        def forward(self, x):\n",
    "            return x * torch.sigmoid(x)\n",
    "        \n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, inp, oup, reduction=4):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "                nn.Linear(oup, _make_divisible(inp // reduction, 8)),\n",
    "                SiLU(),\n",
    "                nn.Linear(_make_divisible(inp // reduction, 8), oup),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y\n",
    "        \n",
    "class MBConv(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, expand_ratio, use_se):\n",
    "        super(MBConv, self).__init__()\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        hidden_dim = round(inp * expand_ratio)\n",
    "        self.identity = stride == 1 and inp == oup\n",
    "        if use_se:\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw\n",
    "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                SiLU(),\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                SiLU(),\n",
    "                SELayer(inp, hidden_dim),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                # fused\n",
    "                nn.Conv2d(inp, hidden_dim, 3, stride, 1, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                SiLU(),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.identity:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "\n",
    "            return self.conv(x)\n",
    "        \n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    DCGAN ENCODER NETWORK\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, imageSize, nz, nc, ngf, ngpu, n_extra_layers=0, add_final_conv=True):\n",
    "        # nz : dimensionality of the latent space潜在空间的维度\n",
    "        # nc : number of image channels\n",
    "        # ndf : channels of middle layers for generator 生成器中间层通道数\n",
    "        # ngpu : number of gpu\n",
    "        # n_extra_layers : extra layers of Encoder and Decoder\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        assert imageSize % 16 == 0, \"imageSize has to be a multiple of 16\"\n",
    "        \n",
    "        self.conv1=conv_3x3_bn(inp=3, oup=64, stride=2)\n",
    "        self.Fused_MBConv_0=MBConv(inp=64, oup=64, stride=1, expand_ratio=1, use_se=0)\n",
    "        self.Fused_MBConv_1=MBConv(inp=64, oup=128, stride=2, expand_ratio=4, use_se=0)\n",
    "        self.Fused_MBConv_2=MBConv(inp=128, oup=256, stride=2, expand_ratio=4, use_se=0)\n",
    "        \n",
    "        self.MBConv_3=MBConv(inp=256, oup=512, stride=2, expand_ratio=4, use_se=1)\n",
    "        self.MBConv_4=MBConv(inp=512, oup=1024, stride=2, expand_ratio=6, use_se=1)\n",
    "        self.MBConv_5=MBConv(inp=1024, oup=2048, stride=2, expand_ratio=6, use_se=1)\n",
    "        self.final=nn.Conv2d(2048, nz, 4, 1, 0, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)#[1, 64, 64, 64]\n",
    "#         print(x.shape)\n",
    "        x = self.Fused_MBConv_0(x)#[1, 64, 64, 64]\n",
    "#         print(x.shape)\n",
    "        x = self.Fused_MBConv_1(x)#1, 128, 32, 32\n",
    "#         print(x.shape)\n",
    "        x=self.Fused_MBConv_2(x)#1, 256, 16, 16\n",
    "#         print(x.shape)\n",
    "        x = self.MBConv_3(x)#1, 256, 16, 16\n",
    "#         print(x.shape)\n",
    "        x = self.MBConv_4(x)\n",
    "        print(x.shape)\n",
    "        x = self.MBConv_5(x)#1, 512, 8, 8\n",
    "        print(x.shape)\n",
    "        x = self.final(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "piano-simpson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024, 8, 8])\n",
      "torch.Size([1, 2048, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "model=Encoder(imageSize=256, nz=100, nc=3, ngf=64, ngpu=1, n_extra_layers=0, add_final_conv=True)\n",
    "input=torch.rand((1,3,256,256))\n",
    "x=model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "young-singing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 1, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "temporal-beatles",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MAdd]: SiLU is not supported!\n",
      "[Flops]: SiLU is not supported!\n",
      "[Memory]: SiLU is not supported!\n",
      "[MAdd]: SiLU is not supported!\n",
      "[Flops]: SiLU is not supported!\n",
      "[Memory]: SiLU is not supported!\n",
      "[MAdd]: SiLU is not supported!\n",
      "[Flops]: SiLU is not supported!\n",
      "[Memory]: SiLU is not supported!\n",
      "[MAdd]: SiLU is not supported!\n",
      "[Flops]: SiLU is not supported!\n",
      "[Memory]: SiLU is not supported!\n",
      "[MAdd]: SiLU is not supported!\n",
      "[Flops]: SiLU is not supported!\n",
      "[Memory]: SiLU is not supported!\n",
      "[MAdd]: SiLU is not supported!\n",
      "[Flops]: SiLU is not supported!\n",
      "[Memory]: SiLU is not supported!\n",
      "[MAdd]: SiLU is not supported!\n",
      "[Flops]: SiLU is not supported!\n",
      "[Memory]: SiLU is not supported!\n",
      "[MAdd]: SiLU is not supported!\n",
      "[Flops]: SiLU is not supported!\n",
      "[Memory]: SiLU is not supported!\n",
      "[MAdd]: SiLU is not supported!\n",
      "[Flops]: SiLU is not supported!\n",
      "[Memory]: SiLU is not supported!\n",
      "[MAdd]: SiLU is not supported!\n",
      "[Flops]: SiLU is not supported!\n",
      "[Memory]: SiLU is not supported!\n",
      "[MAdd]: SiLU is not supported!\n",
      "[Flops]: SiLU is not supported!\n",
      "[Memory]: SiLU is not supported!\n",
      "[MAdd]: SiLU is not supported!\n",
      "[Flops]: SiLU is not supported!\n",
      "[Memory]: SiLU is not supported!\n",
      "[MAdd]: AdaptiveAvgPool2d is not supported!\n",
      "[Flops]: AdaptiveAvgPool2d is not supported!\n",
      "[Memory]: AdaptiveAvgPool2d is not supported!\n",
      "[MAdd]: AdaptiveAvgPool2d is not supported!\n",
      "[Flops]: AdaptiveAvgPool2d is not supported!\n",
      "[Memory]: AdaptiveAvgPool2d is not supported!\n",
      "[MAdd]: SiLU is not supported!\n",
      "[Flops]: SiLU is not supported!\n",
      "[Memory]: SiLU is not supported!\n",
      "[MAdd]: SiLU is not supported!\n",
      "[Flops]: SiLU is not supported!\n",
      "[Memory]: SiLU is not supported!\n",
      "[MAdd]: Sigmoid is not supported!\n",
      "[Flops]: Sigmoid is not supported!\n",
      "[Memory]: Sigmoid is not supported!\n",
      "[MAdd]: Sigmoid is not supported!\n",
      "[Flops]: Sigmoid is not supported!\n",
      "[Memory]: Sigmoid is not supported!\n",
      "[MAdd]: SiLU is not supported!\n",
      "[Flops]: SiLU is not supported!\n",
      "[Memory]: SiLU is not supported!\n",
      "[MAdd]: SiLU is not supported!\n",
      "[Flops]: SiLU is not supported!\n",
      "[Memory]: SiLU is not supported!\n",
      "[MAdd]: SiLU is not supported!\n",
      "[Flops]: SiLU is not supported!\n",
      "[Memory]: SiLU is not supported!\n",
      "[MAdd]: SiLU is not supported!\n",
      "[Flops]: SiLU is not supported!\n",
      "[Memory]: SiLU is not supported!\n",
      "[MAdd]: AdaptiveAvgPool2d is not supported!\n",
      "[Flops]: AdaptiveAvgPool2d is not supported!\n",
      "[Memory]: AdaptiveAvgPool2d is not supported!\n",
      "[MAdd]: AdaptiveAvgPool2d is not supported!\n",
      "[Flops]: AdaptiveAvgPool2d is not supported!\n",
      "[Memory]: AdaptiveAvgPool2d is not supported!\n",
      "[MAdd]: SiLU is not supported!\n",
      "[Flops]: SiLU is not supported!\n",
      "[Memory]: SiLU is not supported!\n",
      "[MAdd]: SiLU is not supported!\n",
      "[Flops]: SiLU is not supported!\n",
      "[Memory]: SiLU is not supported!\n",
      "[MAdd]: Sigmoid is not supported!\n",
      "[Flops]: Sigmoid is not supported!\n",
      "[Memory]: Sigmoid is not supported!\n",
      "[MAdd]: Sigmoid is not supported!\n",
      "[Flops]: Sigmoid is not supported!\n",
      "[Memory]: Sigmoid is not supported!\n",
      "torch.Size([1, 1024, 8, 8])\n",
      "[MAdd]: SiLU is not supported!\n",
      "[Flops]: SiLU is not supported!\n",
      "[Memory]: SiLU is not supported!\n",
      "[MAdd]: SiLU is not supported!\n",
      "[Flops]: SiLU is not supported!\n",
      "[Memory]: SiLU is not supported!\n",
      "[MAdd]: SiLU is not supported!\n",
      "[Flops]: SiLU is not supported!\n",
      "[Memory]: SiLU is not supported!\n",
      "[MAdd]: SiLU is not supported!\n",
      "[Flops]: SiLU is not supported!\n",
      "[Memory]: SiLU is not supported!\n",
      "[MAdd]: AdaptiveAvgPool2d is not supported!\n",
      "[Flops]: AdaptiveAvgPool2d is not supported!\n",
      "[Memory]: AdaptiveAvgPool2d is not supported!\n",
      "[MAdd]: AdaptiveAvgPool2d is not supported!\n",
      "[Flops]: AdaptiveAvgPool2d is not supported!\n",
      "[Memory]: AdaptiveAvgPool2d is not supported!\n",
      "[MAdd]: SiLU is not supported!\n",
      "[Flops]: SiLU is not supported!\n",
      "[Memory]: SiLU is not supported!\n",
      "[MAdd]: SiLU is not supported!\n",
      "[Flops]: SiLU is not supported!\n",
      "[Memory]: SiLU is not supported!\n",
      "[MAdd]: Sigmoid is not supported!\n",
      "[Flops]: Sigmoid is not supported!\n",
      "[Memory]: Sigmoid is not supported!\n",
      "[MAdd]: Sigmoid is not supported!\n",
      "[Flops]: Sigmoid is not supported!\n",
      "[Memory]: Sigmoid is not supported!\n",
      "                    module name   input shape  output shape      params memory(MB)             MAdd            Flops  MemRead(B)  MemWrite(B) duration[%]    MemR+W(B)\n",
      "0                       conv1.0     3 256 256    64 128 128      1728.0       4.00     55,574,528.0     28,311,552.0    793344.0    4194304.0       0.17%    4987648.0\n",
      "1                       conv1.1    64 128 128    64 128 128       128.0       4.00      4,194,304.0      2,097,152.0   4194816.0    4194304.0       0.01%    8389120.0\n",
      "2                       conv1.2    64 128 128    64 128 128         0.0       4.00              0.0              0.0         0.0          0.0       0.07%          0.0\n",
      "3         Fused_MBConv_0.conv.0    64 128 128    64 128 128     36864.0       4.00  1,206,910,976.0    603,979,776.0   4341760.0    4194304.0       0.26%    8536064.0\n",
      "4         Fused_MBConv_0.conv.1    64 128 128    64 128 128       128.0       4.00      4,194,304.0      2,097,152.0   4194816.0    4194304.0       0.01%    8389120.0\n",
      "5         Fused_MBConv_0.conv.2    64 128 128    64 128 128         0.0       4.00              0.0              0.0         0.0          0.0       0.06%          0.0\n",
      "6         Fused_MBConv_0.conv.3    64 128 128    64 128 128      4096.0       4.00    133,169,152.0     67,108,864.0   4210688.0    4194304.0       0.06%    8404992.0\n",
      "7         Fused_MBConv_0.conv.4    64 128 128    64 128 128       128.0       4.00      4,194,304.0      2,097,152.0   4194816.0    4194304.0       0.02%    8389120.0\n",
      "8         Fused_MBConv_1.conv.0    64 128 128   256  64  64    147456.0       4.00  1,206,910,976.0    603,979,776.0   4784128.0    4194304.0       0.27%    8978432.0\n",
      "9         Fused_MBConv_1.conv.1   256  64  64   256  64  64       512.0       4.00      4,194,304.0      2,097,152.0   4196352.0    4194304.0       0.01%    8390656.0\n",
      "10        Fused_MBConv_1.conv.2   256  64  64   256  64  64         0.0       4.00              0.0              0.0         0.0          0.0       0.06%          0.0\n",
      "11        Fused_MBConv_1.conv.3   256  64  64   128  64  64     32768.0       2.00    267,911,168.0    134,217,728.0   4325376.0    2097152.0       0.06%    6422528.0\n",
      "12        Fused_MBConv_1.conv.4   128  64  64   128  64  64       256.0       2.00      2,097,152.0      1,048,576.0   2098176.0    2097152.0       0.01%    4195328.0\n",
      "13        Fused_MBConv_2.conv.0   128  64  64   512  32  32    589824.0       2.00  1,207,435,264.0    603,979,776.0   4456448.0    2097152.0       0.27%    6553600.0\n",
      "14        Fused_MBConv_2.conv.1   512  32  32   512  32  32      1024.0       2.00      2,097,152.0      1,048,576.0   2101248.0    2097152.0       0.00%    4198400.0\n",
      "15        Fused_MBConv_2.conv.2   512  32  32   512  32  32         0.0       2.00              0.0              0.0         0.0          0.0       0.06%          0.0\n",
      "16        Fused_MBConv_2.conv.3   512  32  32   256  32  32    131072.0       1.00    268,173,312.0    134,217,728.0   2621440.0    1048576.0       0.06%    3670016.0\n",
      "17        Fused_MBConv_2.conv.4   256  32  32   256  32  32       512.0       1.00      1,048,576.0        524,288.0   1050624.0    1048576.0       0.00%    2099200.0\n",
      "18              MBConv_3.conv.0   256  32  32  1024  32  32    262144.0       4.00    535,822,336.0    268,435,456.0   2097152.0    4194304.0       0.11%    6291456.0\n",
      "19              MBConv_3.conv.1  1024  32  32  1024  32  32      2048.0       4.00      4,194,304.0      2,097,152.0   4202496.0    4194304.0       0.02%    8396800.0\n",
      "20              MBConv_3.conv.2  1024  32  32  1024  32  32         0.0       4.00              0.0              0.0         0.0          0.0       0.05%          0.0\n",
      "21              MBConv_3.conv.3  1024  32  32  1024  16  16      9216.0       1.00      4,456,448.0      2,359,296.0   4231168.0    1048576.0       0.26%    5279744.0\n",
      "22              MBConv_3.conv.4  1024  16  16  1024  16  16      2048.0       1.00      1,048,576.0        524,288.0   1056768.0    1048576.0       0.00%    2105344.0\n",
      "23              MBConv_3.conv.5  1024  16  16  1024  16  16         0.0       1.00              0.0              0.0         0.0          0.0       0.06%          0.0\n",
      "24     MBConv_3.conv.6.avg_pool  1024  16  16  1024   1   1         0.0       0.00              0.0              0.0         0.0          0.0       0.06%          0.0\n",
      "25         MBConv_3.conv.6.fc.0          1024            64     65600.0       0.00        131,008.0         65,536.0    266496.0        256.0       0.06%     266752.0\n",
      "26         MBConv_3.conv.6.fc.1            64            64         0.0       0.00              0.0              0.0         0.0          0.0       0.00%          0.0\n",
      "27         MBConv_3.conv.6.fc.2            64          1024     66560.0       0.00        130,048.0         65,536.0    266496.0       4096.0       0.06%     270592.0\n",
      "28         MBConv_3.conv.6.fc.3          1024          1024         0.0       0.00              0.0              0.0         0.0          0.0       0.00%          0.0\n",
      "29              MBConv_3.conv.7  1024  16  16   512  16  16    524288.0       0.50    268,304,384.0    134,217,728.0   3145728.0     524288.0       0.07%    3670016.0\n",
      "30              MBConv_3.conv.8   512  16  16   512  16  16      1024.0       0.50        524,288.0        262,144.0    528384.0     524288.0       0.00%    1052672.0\n",
      "31              MBConv_4.conv.0   512  16  16  3072  16  16   1572864.0       3.00    804,519,936.0    402,653,184.0   6815744.0    3145728.0       0.13%    9961472.0\n",
      "32              MBConv_4.conv.1  3072  16  16  3072  16  16      6144.0       3.00      3,145,728.0      1,572,864.0   3170304.0    3145728.0       0.01%    6316032.0\n",
      "33              MBConv_4.conv.2  3072  16  16  3072  16  16         0.0       3.00              0.0              0.0         0.0          0.0       0.05%          0.0\n",
      "34              MBConv_4.conv.3  3072  16  16  3072   8   8     27648.0       0.75      3,342,336.0      1,769,472.0   3256320.0     786432.0      95.57%    4042752.0\n",
      "35              MBConv_4.conv.4  3072   8   8  3072   8   8      6144.0       0.75        786,432.0        393,216.0    811008.0     786432.0       0.01%    1597440.0\n",
      "36              MBConv_4.conv.5  3072   8   8  3072   8   8         0.0       0.75              0.0              0.0         0.0          0.0       0.01%          0.0\n",
      "37     MBConv_4.conv.6.avg_pool  3072   8   8  3072   1   1         0.0       0.01              0.0              0.0         0.0          0.0       0.00%          0.0\n",
      "38         MBConv_4.conv.6.fc.0          3072           128    393344.0       0.00        786,304.0        393,216.0   1585664.0        512.0       0.01%    1586176.0\n",
      "39         MBConv_4.conv.6.fc.1           128           128         0.0       0.00              0.0              0.0         0.0          0.0       0.00%          0.0\n",
      "40         MBConv_4.conv.6.fc.2           128          3072    396288.0       0.01        783,360.0        393,216.0   1585664.0      12288.0       0.01%    1597952.0\n",
      "41         MBConv_4.conv.6.fc.3          3072          3072         0.0       0.01              0.0              0.0         0.0          0.0       0.00%          0.0\n",
      "42              MBConv_4.conv.7  3072   8   8  1024   8   8   3145728.0       0.25    402,587,648.0    201,326,592.0  13369344.0     262144.0       0.08%   13631488.0\n",
      "43              MBConv_4.conv.8  1024   8   8  1024   8   8      2048.0       0.25        262,144.0        131,072.0    270336.0     262144.0       0.00%     532480.0\n",
      "44              MBConv_5.conv.0  1024   8   8  6144   8   8   6291456.0       1.50    804,913,152.0    402,653,184.0  25427968.0    1572864.0       0.16%   27000832.0\n",
      "45              MBConv_5.conv.1  6144   8   8  6144   8   8     12288.0       1.50      1,572,864.0        786,432.0   1622016.0    1572864.0       0.01%    3194880.0\n",
      "46              MBConv_5.conv.2  6144   8   8  6144   8   8         0.0       1.50              0.0              0.0         0.0          0.0       0.05%          0.0\n",
      "47              MBConv_5.conv.3  6144   8   8  6144   4   4     55296.0       0.38      1,671,168.0        884,736.0   1794048.0     393216.0       1.39%    2187264.0\n",
      "48              MBConv_5.conv.4  6144   4   4  6144   4   4     12288.0       0.38        393,216.0        196,608.0    442368.0     393216.0       0.00%     835584.0\n",
      "49              MBConv_5.conv.5  6144   4   4  6144   4   4         0.0       0.38              0.0              0.0         0.0          0.0       0.04%          0.0\n",
      "50     MBConv_5.conv.6.avg_pool  6144   4   4  6144   1   1         0.0       0.02              0.0              0.0         0.0          0.0       0.03%          0.0\n",
      "51         MBConv_5.conv.6.fc.0          6144           256   1573120.0       0.00      3,145,472.0      1,572,864.0   6317056.0       1024.0       0.02%    6318080.0\n",
      "52         MBConv_5.conv.6.fc.1           256           256         0.0       0.00              0.0              0.0         0.0          0.0       0.00%          0.0\n",
      "53         MBConv_5.conv.6.fc.2           256          6144   1579008.0       0.02      3,139,584.0      1,572,864.0   6317056.0      24576.0       0.02%    6341632.0\n",
      "54         MBConv_5.conv.6.fc.3          6144          6144         0.0       0.02              0.0              0.0         0.0          0.0       0.00%          0.0\n",
      "55              MBConv_5.conv.7  6144   4   4  2048   4   4  12582912.0       0.12    402,620,416.0    201,326,592.0  50724864.0     131072.0       0.18%   50855936.0\n",
      "56              MBConv_5.conv.8  2048   4   4  2048   4   4      4096.0       0.12        131,072.0         65,536.0    147456.0     131072.0       0.00%     278528.0\n",
      "57                        final  2048   4   4   100   1   1   3276800.0       0.00      6,553,500.0      3,276,800.0  13238272.0        400.0       0.05%   13238672.0\n",
      "total                                                        32816896.0      89.75  7,623,071,196.0  3,815,800,832.0  13238272.0        400.0     100.00%  268454800.0\n",
      "======================================================================================================================================================================\n",
      "Total params: 32,816,896\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Total memory: 89.75MB\n",
      "Total MAdd: 7.62GMAdd\n",
      "Total Flops: 3.82GFlops\n",
      "Total MemR+W: 256.02MB\n",
      "\n",
      "Number of parameters: 32.82M\n"
     ]
    }
   ],
   "source": [
    "model=Encoder(imageSize=256, nz=100, nc=3, ngf=64, ngpu=1, n_extra_layers=0, add_final_conv=True)\n",
    "# input=torch.rand((1,3,256,256))\n",
    "# x=model(input)\n",
    "stat(model, (3, 256, 256))\n",
    "total = sum([param.nelement() for param in model.parameters()])\n",
    "print(\"Number of parameters: %.2fM\" % (total/1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structural-seafood",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchstat import stat\n",
    "from torchvision.models import resnet50, resnet101, resnet152, resnext101_32x8d\n",
    " \n",
    "model = resnet50()\n",
    "stat(model, (3, 224, 224))\n",
    "total = sum([param.nelement() for param in model.parameters()])\n",
    "print(\"Number of parameters: %.2fM\" % (total/1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "destroyed-transmission",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    DCGAN ENCODER NETWORK\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, imageSize, nz, nc, ngf, ngpu, n_extra_layers=0, add_final_conv=True):\n",
    "        # nz : dimensionality of the latent space潜在空间的维度\n",
    "        # nc : number of image channels\n",
    "        # ndf : channels of middle layers for generator 生成器中间层通道数\n",
    "        # ngpu : number of gpu\n",
    "        # n_extra_layers : extra layers of Encoder and Decoder\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        assert imageSize % 16 == 0, \"imageSize has to be a multiple of 16\"\n",
    "\n",
    "        \n",
    "        self.initial0 = nn.Sequential(\n",
    "            nn.Conv2d(nc, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "        self.pyramid0 = nn.Sequential(\n",
    "            nn.Conv2d(ngf, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        self.pyramid1 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        self.pyramid2 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        self.pyramid3 = nn.Sequential(\n",
    "            nn.Conv2d(512, 1024, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        self.pyramid4 = nn.Sequential(\n",
    "            nn.Conv2d(1024, 2048, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(2048),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        if add_final_conv:\n",
    "            self.final_conv=nn.Conv2d(2048, nz, 4, 1, 0, bias=False)\n",
    "            \n",
    "    def forward(self, input):\n",
    "#         x=[]\n",
    "        \n",
    "        output=self.initial0(input)\n",
    "#         x.append(output)\n",
    "        \n",
    "        output=self.pyramid0(output)\n",
    "#         x.append(output)\n",
    "        \n",
    "        output=self.pyramid1(output)\n",
    "#         x.append(output)\n",
    "        \n",
    "        output=self.pyramid2(output)\n",
    "#         x.append(output)\n",
    "        \n",
    "        output=self.pyramid3(output)\n",
    "        output=self.pyramid4(output)\n",
    "#         x.append(output)\n",
    "        y=self.final_conv(output)    \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "equivalent-territory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MAdd]: LeakyReLU is not supported!\n",
      "[MAdd]: LeakyReLU is not supported!\n",
      "[MAdd]: LeakyReLU is not supported!\n",
      "[MAdd]: LeakyReLU is not supported!\n",
      "[MAdd]: LeakyReLU is not supported!\n",
      "[MAdd]: LeakyReLU is not supported!\n",
      "[MAdd]: LeakyReLU is not supported!\n",
      "[MAdd]: LeakyReLU is not supported!\n",
      "[MAdd]: LeakyReLU is not supported!\n",
      "[MAdd]: LeakyReLU is not supported!\n",
      "      module name   input shape  output shape      params memory(MB)             MAdd            Flops   MemRead(B)  MemWrite(B) duration[%]    MemR+W(B)\n",
      "0      initial0.0     3 256 256    64 128 128      3072.0       4.00     99,614,720.0     50,331,648.0     798720.0    4194304.0      23.29%    4993024.0\n",
      "1      initial0.1    64 128 128    64 128 128       128.0       4.00      4,194,304.0      2,097,152.0    4194816.0    4194304.0       4.09%    8389120.0\n",
      "2      initial0.2    64 128 128    64 128 128         0.0       4.00      1,048,576.0      1,048,576.0    4194304.0    4194304.0       0.42%    8388608.0\n",
      "3      pyramid0.0    64 128 128   128  64  64    131072.0       2.00  1,073,217,536.0    536,870,912.0    4718592.0    2097152.0       4.15%    6815744.0\n",
      "4      pyramid0.1   128  64  64   128  64  64       256.0       2.00      2,097,152.0      1,048,576.0    2098176.0    2097152.0       0.97%    4195328.0\n",
      "5      pyramid0.2   128  64  64   128  64  64         0.0       2.00              0.0        524,288.0    2097152.0    2097152.0       0.67%    4194304.0\n",
      "6      pyramid1.0   128  64  64   256  32  32    524288.0       1.00  1,073,479,680.0    536,870,912.0    4194304.0    1048576.0       3.79%    5242880.0\n",
      "7      pyramid1.1   256  32  32   256  32  32       512.0       1.00      1,048,576.0        524,288.0    1050624.0    1048576.0       0.66%    2099200.0\n",
      "8      pyramid1.2   256  32  32   256  32  32         0.0       1.00              0.0        262,144.0    1048576.0    1048576.0       0.57%    2097152.0\n",
      "9      pyramid2.0   256  32  32   512  16  16   2097152.0       0.50  1,073,610,752.0    536,870,912.0    9437184.0     524288.0       4.17%    9961472.0\n",
      "10     pyramid2.1   512  16  16   512  16  16      1024.0       0.50        524,288.0        262,144.0     528384.0     524288.0       0.64%    1052672.0\n",
      "11     pyramid2.2   512  16  16   512  16  16         0.0       0.50              0.0        131,072.0     524288.0     524288.0       0.55%    1048576.0\n",
      "12     pyramid3.0   512  16  16  1024   8   8   8388608.0       0.25  1,073,676,288.0    536,870,912.0   34078720.0     262144.0      11.18%   34340864.0\n",
      "13     pyramid3.1  1024   8   8  1024   8   8      2048.0       0.25        262,144.0        131,072.0     270336.0     262144.0       0.46%     532480.0\n",
      "14     pyramid3.2  1024   8   8  1024   8   8         0.0       0.25              0.0         65,536.0     262144.0     262144.0       0.60%     524288.0\n",
      "15     pyramid4.0  1024   8   8  2048   4   4  33554432.0       0.12  1,073,709,056.0    536,870,912.0  134479872.0     131072.0      37.17%  134610944.0\n",
      "16     pyramid4.1  2048   4   4  2048   4   4      4096.0       0.12        131,072.0         65,536.0     147456.0     131072.0       0.49%     278528.0\n",
      "17     pyramid4.2  2048   4   4  2048   4   4         0.0       0.12              0.0         32,768.0     131072.0     131072.0       0.75%     262144.0\n",
      "18     final_conv  2048   4   4   100   1   1   3276800.0       0.00      6,553,500.0      3,276,800.0   13238272.0        400.0       5.38%   13238672.0\n",
      "total                                          47983488.0      23.63  5,483,167,644.0  2,744,156,160.0   13238272.0        400.0     100.00%  242266000.0\n",
      "=========================================================================================================================================================\n",
      "Total params: 47,983,488\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Total memory: 23.63MB\n",
      "Total MAdd: 5.48GMAdd\n",
      "Total Flops: 2.74GFlops\n",
      "Total MemR+W: 231.04MB\n",
      "\n",
      "Number of parameters: 47.98M\n"
     ]
    }
   ],
   "source": [
    "model=Encoder(imageSize=256, nz=100, nc=3, ngf=64, ngpu=1, n_extra_layers=0, add_final_conv=True)\n",
    "# input=torch.rand((1,3,256,256))\n",
    "# x=model(input)\n",
    "stat(model, (3, 256, 256))\n",
    "total = sum([param.nelement() for param in model.parameters()])\n",
    "print(\"Number of parameters: %.2fM\" % (total/1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-steps",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
