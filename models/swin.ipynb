{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "antique-lawrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "from einops import rearrange\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "from swin_test import SwinTransformerBlock,BasicLayer_up,PatchMerging,PatchExpand,FinalPatchExpand_X4,BasicLayer,PatchEmbed,WindowAttention,Mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "norman-failing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from memory_module import MemModule_window,MemModule,MemModule1_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "controlled-contest",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformerSys(nn.Module):\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=4, in_chans=3, num_classes=3,\n",
    "                 embed_dim=96, depths=[2, 2, 2, 2], depths_decoder=[1, 2, 2, 2], num_heads=[3, 6, 12, 24],\n",
    "                 window_size=7, mlp_ratio=4., qkv_bias=True, qk_scale=None,\n",
    "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n",
    "                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,\n",
    "                 use_checkpoint=False, final_upsample=\"expand_first\", **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        print(\"SwinTransformerSys expand initial----depths:{};depths_decoder:{};drop_path_rate:{};num_classes:{}\".format(depths,\n",
    "        depths_decoder,drop_path_rate,num_classes))\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ape = ape\n",
    "        self.patch_norm = patch_norm\n",
    "        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n",
    "        self.num_features_up = int(embed_dim * 2)\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.final_upsample = final_upsample\n",
    "\n",
    "        # split image into non-overlapping patches\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer if self.patch_norm else None)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        patches_resolution = self.patch_embed.patches_resolution\n",
    "        self.patches_resolution = patches_resolution\n",
    "\n",
    "        # absolute position embedding\n",
    "        if self.ape:\n",
    "            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "            trunc_normal_(self.absolute_pos_embed, std=.02)\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # stochastic depth\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
    "\n",
    "        # build encoder and bottleneck layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer),\n",
    "                               input_resolution=(patches_resolution[0] // (2 ** i_layer),\n",
    "                                                 patches_resolution[1] // (2 ** i_layer)),\n",
    "                               depth=depths[i_layer],\n",
    "                               num_heads=num_heads[i_layer],\n",
    "                               window_size=window_size,\n",
    "                               mlp_ratio=self.mlp_ratio,\n",
    "                               qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                               drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "                               drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n",
    "                               norm_layer=norm_layer,\n",
    "                               downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,\n",
    "                               use_checkpoint=use_checkpoint)\n",
    "            self.layers.append(layer)\n",
    "        \n",
    "        # build decoder layers\n",
    "        self.layers_up = nn.ModuleList()\n",
    "        self.concat_back_dim = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            concat_linear = nn.Linear(2*int(embed_dim*2**(self.num_layers-1-i_layer)),\n",
    "            int(embed_dim*2**(self.num_layers-1-i_layer))) if i_layer > 0 else nn.Identity()\n",
    "            if i_layer ==0 :\n",
    "                layer_up = PatchExpand(input_resolution=(patches_resolution[0] // (2 ** (self.num_layers-1-i_layer)),\n",
    "                patches_resolution[1] // (2 ** (self.num_layers-1-i_layer))), dim=int(embed_dim * 2 ** (self.num_layers-1-i_layer)), dim_scale=2, norm_layer=norm_layer)\n",
    "            else:\n",
    "                layer_up = BasicLayer_up(dim=int(embed_dim * 2 ** (self.num_layers-1-i_layer)),\n",
    "                                input_resolution=(patches_resolution[0] // (2 ** (self.num_layers-1-i_layer)),\n",
    "                                                    patches_resolution[1] // (2 ** (self.num_layers-1-i_layer))),\n",
    "                                depth=depths[(self.num_layers-1-i_layer)],\n",
    "                                num_heads=num_heads[(self.num_layers-1-i_layer)],\n",
    "                                window_size=window_size,\n",
    "                                mlp_ratio=self.mlp_ratio,\n",
    "                                qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                                drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "                                drop_path=dpr[sum(depths[:(self.num_layers-1-i_layer)]):sum(depths[:(self.num_layers-1-i_layer) + 1])],\n",
    "                                norm_layer=norm_layer,\n",
    "                                upsample=PatchExpand if (i_layer < self.num_layers - 1) else None,\n",
    "                                use_checkpoint=use_checkpoint)\n",
    "            self.layers_up.append(layer_up)\n",
    "            self.concat_back_dim.append(concat_linear)\n",
    "\n",
    "        self.norm = norm_layer(self.num_features)\n",
    "        self.norm_up= norm_layer(self.embed_dim)\n",
    "\n",
    "        if self.final_upsample == \"expand_first\":\n",
    "            print(\"---final upsample expand_first---\")\n",
    "            self.up = FinalPatchExpand_X4(input_resolution=(img_size//patch_size,img_size//patch_size),dim_scale=4,dim=embed_dim)\n",
    "            self.output = nn.Conv2d(in_channels=embed_dim,out_channels=self.num_classes,kernel_size=1,bias=False)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        ###################################\n",
    "        mem_dim=2000\n",
    "        shrink_thres=0.0005\n",
    "        self.mem_rep0_ = MemModule_window(mem_dim=mem_dim, fea_dim=96, window=1,c_num=8,shrink_thres =shrink_thres)\n",
    "        self.mem_rep0 = MemModule_window(mem_dim=mem_dim, fea_dim=192, window=2,c_num=8,shrink_thres =shrink_thres)\n",
    "        ############               \n",
    "        self.mem_rep1 = MemModule_window(mem_dim=mem_dim, fea_dim=384, window=4,c_num=8,shrink_thres =shrink_thres)\n",
    "        ############                       \n",
    "        self.mem_rep2 = MemModule_window(mem_dim=mem_dim, fea_dim=768, window=8,c_num=8,shrink_thres =shrink_thres)\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'absolute_pos_embed'}\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay_keywords(self):\n",
    "        return {'relative_position_bias_table'}\n",
    "\n",
    "    #Encoder and Bottleneck\n",
    "    def forward_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        if self.ape:\n",
    "            x = x + self.absolute_pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "        x_downsample = []\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x_downsample.append(x)\n",
    "            \n",
    "            x = layer(x)\n",
    "#             print(\"x2.shape\",x.shape)\n",
    "\n",
    "        x = self.norm(x)  # B L C\n",
    "  \n",
    "        return x, x_downsample\n",
    "\n",
    "    #Dencoder and Skip connection\n",
    "    def forward_up_features(self, x, x_downsample):\n",
    "        res=[]\n",
    "        i=0\n",
    "        for inx, layer_up in enumerate(self.layers_up):\n",
    "            if inx == 0:\n",
    "                print(\"x0.shape\",x.shape)\n",
    "                b,h_w,c=x.shape\n",
    "                x=x.view(b,int(h_w**0.5),int(h_w**0.5),c).permute(0, 3, 1, 2).contiguous()\n",
    "                res0=self.mem_rep0_(x)\n",
    "                res.append(res0)\n",
    "                x=res0[\"output\"].view(b,c,-1).permute(0, 2, 1).contiguous()\n",
    "                x = layer_up(x)\n",
    "                \n",
    "            else:\n",
    "                print(\"x.shape\",x.shape)\n",
    "                b,h_w,c=x.shape\n",
    "                x=x.view(b,int(h_w**0.5),int(h_w**0.5),c).permute(0, 3, 1, 2).contiguous()\n",
    "                print(\"x.shape1\",x.shape)\n",
    "                if i==0:\n",
    "                    res0=self.mem_rep0(x)\n",
    "                    print(0)\n",
    "                elif i==1:\n",
    "                    res0=self.mem_rep1(x)\n",
    "                    print(1)\n",
    "                    \n",
    "                else:\n",
    "                    res0=self.mem_rep2(x)\n",
    "                    print(2)\n",
    "                    \n",
    "                res.append(res0)\n",
    "                i+=1\n",
    "                x=res0[\"output\"].view(b,c,-1).permute(0, 2, 1).contiguous()\n",
    "                x = torch.cat([x,x_downsample[3-inx]],-1)###############\n",
    "                x = self.concat_back_dim[inx](x)\n",
    "                x = layer_up(x)\n",
    "\n",
    "        x = self.norm_up(x)  # B L C\n",
    "  \n",
    "        return x,res\n",
    "\n",
    "    def up_x4(self, x):\n",
    "        H, W = self.patches_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H*W, \"input features has wrong size\"\n",
    "\n",
    "        if self.final_upsample==\"expand_first\":\n",
    "            x = self.up(x)\n",
    "            x = x.view(B,4*H,4*W,-1)\n",
    "            x = x.permute(0,3,1,2) #B,C,H,W\n",
    "#             print(\"x3.shape\",x.shape)\n",
    "            x = self.output(x)\n",
    "            \n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, x_downsample = self.forward_features(x)\n",
    "        x,res = self.forward_up_features(x,x_downsample)\n",
    "#         print( \"x.shape\",x.shape)\n",
    "        x = self.up_x4(x)\n",
    "#         print( \"x.shape\",x.shape)\n",
    "        down=[]\n",
    "        for i in range(len(x_downsample)):\n",
    "            b,wh,c=x_downsample[i].shape\n",
    "            x_downsample[i] = x_downsample[i].view(1,int(wh**0.5),int(wh**0.5),-1)\n",
    "#             y = y.permute(0,3,1,2) #B,C,H,W\n",
    "#             print(\"y.shape\",y.shape)\n",
    "        return x,x_downsample,res\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        flops += self.patch_embed.flops()\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            flops += layer.flops()\n",
    "        flops += self.num_features * self.patches_resolution[0] * self.patches_resolution[1] // (2 ** self.num_layers)\n",
    "        flops += self.num_features * self.num_classes\n",
    "        return flops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "interior-sympathy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SwinTransformerSys expand initial----depths:[2, 2, 2, 2];depths_decoder:[1, 2, 2, 2];drop_path_rate:0.1;num_classes:3\n",
      "---final upsample expand_first---\n",
      "x0.shape torch.Size([1, 49, 768])\n",
      "x.shape torch.Size([1, 196, 384])\n",
      "x.shape1 torch.Size([1, 384, 14, 14])\n",
      "0\n",
      "x.shape torch.Size([1, 784, 192])\n",
      "x.shape1 torch.Size([1, 192, 28, 28])\n",
      "1\n",
      "x.shape torch.Size([1, 3136, 96])\n",
      "x.shape1 torch.Size([1, 96, 56, 56])\n",
      "2\n",
      "output.shape torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    x=torch.randn(1,3,224,224)\n",
    "    model= SwinTransformerSys()\n",
    "    output,down,res=model(x)\n",
    "    print('output.shape',output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-suffering",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "enclosed-chambers",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def window_partition(x, window_size):\n",
    "   \n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "   \n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "strange-carolina",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetD, self).__init__()\n",
    "        ngf = 48\n",
    "        nc=3\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(nc, ngf, 4, 2, 1, bias=False),\n",
    "\n",
    "            nn.LeakyReLU(0.2, inplace=True))\n",
    "            \n",
    "        self.mode2 = nn.Sequential(    nn.Conv2d(ngf, ngf << 1, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf << 1),       # 128\n",
    "\n",
    "            nn.LeakyReLU(0.2, inplace=True),)\n",
    "        \n",
    "        self.mode3 = nn.Sequential(     nn.Conv2d(ngf << 1, ngf << 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf << 2),       # 256\n",
    "\n",
    "            nn.LeakyReLU(0.2, inplace=True),)\n",
    "        self.mode4 = nn.Sequential( nn.Conv2d(ngf << 2, ngf << 3, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf << 3),       # 512\n",
    "            \n",
    "            nn.LeakyReLU(0.2, inplace=True),)\n",
    "        self.mode5 = nn.Sequential(     nn.Conv2d(ngf << 3, ngf << 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf << 4),       # 1024\n",
    "            \n",
    "            nn.LeakyReLU(0.2, inplace=True),)\n",
    "        self.mode6 = nn.Sequential(     nn.Conv2d(ngf << 4, ngf << 5, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf << 5), \n",
    "            \n",
    "            nn.LeakyReLU(0.2, inplace=True),)\n",
    "        self.mode7 = nn.Sequential(     nn.Conv2d(ngf << 5, 100, 3, 1, 0, bias=False),       # 512\n",
    "            nn.BatchNorm2d(100),       # 100\n",
    "        )\n",
    "\n",
    "        self.classify = nn.Sequential(\n",
    "            nn.Conv2d(100, 1, 3, 1, 1, bias=False),       # 512\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        print(\"x1.shape\",x.shape)\n",
    "        x = self.mode2(x)\n",
    "        print(\"x2.shape\",x.shape)\n",
    "        x = self.mode3(x)\n",
    "        print(\"x3.shape\",x.shape)\n",
    "        x = self.mode4(x)\n",
    "        print(\"x4.shape\",x.shape)\n",
    "        x = self.mode5(x)\n",
    "        print(\"x5.shape\",x.shape)\n",
    "        x = self.mode6(x)\n",
    "        print(\"x6.shape\",x.shape)\n",
    "        feature = self.mode7(x)\n",
    "        print(\"x7.shape\",feature.shape)\n",
    "        \n",
    "        classification = self.classify(feature)\n",
    "        return classification.view(-1, 1).squeeze(1), feature    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "shared-techno",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1.shape torch.Size([2, 48, 112, 112])\n",
      "x2.shape torch.Size([2, 96, 56, 56])\n",
      "x3.shape torch.Size([2, 192, 28, 28])\n",
      "x4.shape torch.Size([2, 384, 14, 14])\n",
      "x5.shape torch.Size([2, 768, 7, 7])\n",
      "x6.shape torch.Size([2, 1536, 3, 3])\n",
      "x7.shape torch.Size([2, 100, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    x1=torch.randn(2,3,224,224)\n",
    "    model1= NetD()\n",
    "    classification,feature=model1(x1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "sustainable-ethernet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "animated-diversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetD, self).__init__()\n",
    "        ngf = 64\n",
    "        nc=3\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(nc, ngf, 4, 2, 1, bias=False),\n",
    "\n",
    "            nn.LeakyReLU(0.2, inplace=True))\n",
    "            \n",
    "        self.mode2 = nn.Sequential(    nn.Conv2d(ngf, ngf << 1, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf << 1),       # 128\n",
    "\n",
    "            nn.LeakyReLU(0.2, inplace=True),)\n",
    "        \n",
    "        self.mode3 = nn.Sequential(     nn.Conv2d(ngf << 1, ngf << 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf << 2),       # 256\n",
    "\n",
    "            nn.LeakyReLU(0.2, inplace=True),)\n",
    "        self.mode4 = nn.Sequential( nn.Conv2d(ngf << 2, ngf << 3, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf << 3),       # 512\n",
    "            \n",
    "            nn.LeakyReLU(0.2, inplace=True),)\n",
    "        self.mode5 = nn.Sequential(     nn.Conv2d(ngf << 3, ngf << 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf << 4),       # 1024\n",
    "            \n",
    "            nn.LeakyReLU(0.2, inplace=True),)\n",
    "        self.mode6 = nn.Sequential(     nn.Conv2d(ngf << 4, ngf << 5, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf << 5), \n",
    "            \n",
    "            nn.LeakyReLU(0.2, inplace=True),)\n",
    "        self.mode7 = nn.Sequential(     nn.Conv2d(ngf << 5, 100, 4, 1, 0, bias=False),       # 512\n",
    "            nn.BatchNorm2d(100),       # 100\n",
    "        )\n",
    "\n",
    "        self.classify = nn.Sequential(\n",
    "            nn.Conv2d(100, 1, 3, 1, 1, bias=False),       # 512\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        print(\"x1.shape\",x.shape)\n",
    "        x = self.mode2(x)\n",
    "        print(\"x2.shape\",x.shape)\n",
    "        x = self.mode3(x)\n",
    "        print(\"x3.shape\",x.shape)\n",
    "        x = self.mode4(x)\n",
    "        print(\"x4.shape\",x.shape)\n",
    "        x = self.mode5(x)\n",
    "        print(\"x5.shape\",x.shape)\n",
    "        x = self.mode6(x)\n",
    "        print(\"x6.shape\",x.shape)\n",
    "        feature = self.mode7(x)\n",
    "        print(\"x7.shape\",feature.shape)\n",
    "        \n",
    "        classification = self.classify(feature)\n",
    "        return classification.view(-1, 1).squeeze(1), feature    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "still-variance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1.shape torch.Size([2, 64, 128, 128])\n",
      "x2.shape torch.Size([2, 128, 64, 64])\n",
      "x3.shape torch.Size([2, 256, 32, 32])\n",
      "x4.shape torch.Size([2, 512, 16, 16])\n",
      "x5.shape torch.Size([2, 1024, 8, 8])\n",
      "x6.shape torch.Size([2, 2048, 4, 4])\n",
      "x7.shape torch.Size([2, 100, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    x1=torch.randn(2,3,256,256)\n",
    "    model1= NetD()\n",
    "    classification,feature=model1(x1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "driven-jacksonville",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subject-painting",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
