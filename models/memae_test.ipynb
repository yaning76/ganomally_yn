{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "comparative-isaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, print_function\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "digital-presence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MemModule1_new  h*w h/window*w/window\n",
    "# MemModule_window  widow*window*c  (widow*window*c)/c_size       window=2,c_size=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "emotional-contamination",
   "metadata": {},
   "outputs": [],
   "source": [
    "#最终版\n",
    "class MemModule1_new(nn.Module):\n",
    "    def __init__(self,mem_dim,fea_dim,window,shrink_thres=0.0025, device='cuda'):\n",
    "        super(MemModule1_new, self).__init__()\n",
    "        self.mem_dim = mem_dim\n",
    "        self.fea_dim = fea_dim\n",
    "        self.shrink_thres = shrink_thres\n",
    "        self.memory = MemoryUnit(self.mem_dim, self.fea_dim, self.shrink_thres)\n",
    "        self.window=window\n",
    "\n",
    "    def forward(self, x):\n",
    "        s = x.data.shape\n",
    "        x = x.view(s[1]*s[0]*self.window*self.window, -1)\n",
    "        y_and = self.memory(x)\n",
    "        y = y_and['output']\n",
    "#         print('y',y.shape)\n",
    "        att = y_and['att']\n",
    "#         print('att',att.shape)\n",
    "        \n",
    "        y = y.view(s[0], s[1], s[2], s[3])\n",
    "        att = att.view(s[0]*s[1],self.window,self.window,self.mem_dim)\n",
    "        att = att.permute(0, 3, 1, 2)\n",
    "        print('att',att.shape)\n",
    "        \n",
    "#         att = att.view(s[0]* s[1],self.window, self.window, self.mem_dim)\n",
    "        return {'output': y, 'att': att}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "described-departure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "att torch.Size([512, 2000, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 256, 32, 32]), torch.Size([512, 2000, 2, 2]))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fea_dim=h/window*w/window\n",
    "model=MemModule1_new(mem_dim=2000,fea_dim=256,window=2)\n",
    "x=torch.rand((2,256,32,32))\n",
    "out=model(x)\n",
    "out['output'].shape,out['att'].shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "hearing-poetry",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemModule1(nn.Module):\n",
    "    def __init__(self, mem_dim, fea_dim, shrink_thres=0.0025, device='cuda'):\n",
    "        super(MemModule1, self).__init__()\n",
    "        self.mem_dim = mem_dim\n",
    "        self.fea_dim = fea_dim\n",
    "        self.shrink_thres = shrink_thres\n",
    "        self.memory = MemoryUnit(self.mem_dim, self.fea_dim, self.shrink_thres)\n",
    "\n",
    "    def forward(self, x):\n",
    "        s = x.data.shape\n",
    "#         x = input.permute(0, 2, 3, 1).contiguous()\n",
    "#         print(x.shape)\n",
    "        x = x.view(s[1]*s[0], -1)\n",
    "#         print(x.shape)\n",
    "#         print(\"self.fea_dim.shape\",self.fea_dim)\n",
    "        #\n",
    "        y_and = self.memory(x)\n",
    "        \n",
    "        #\n",
    "        y = y_and['output']\n",
    "        print('y',y.shape)\n",
    "        att = y_and['att']\n",
    "        print('att',att.shape)\n",
    "        \n",
    "        y = y.view(s[0], s[1], s[2], s[3])\n",
    "#         print( y.shape)\n",
    "#         y = y.permute(0, 3, 1, 2)\n",
    "#         print( y.shape)\n",
    "        att = att.view(s[0],s[1],self.mem_dim)\n",
    "#         print( att.shape)\n",
    "#         att = att.permute(0, 3, 1, 2)\n",
    "#         print( att.shape)\n",
    "\n",
    "        return {'output': y, 'att': att}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "sublime-cradle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y torch.Size([512, 1024])\n",
      "att torch.Size([512, 2000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 256, 32, 32]), torch.Size([2, 256, 2000]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=MemModule1(mem_dim=2000,fea_dim=1024)\n",
    "x=torch.rand((2,256,32,32))\n",
    "out=model(x)\n",
    "out['output'].shape,out['att'].shape,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "robust-alcohol",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "compressed-prompt",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_shrink_relu(input, lambd=0, epsilon=1e-12):\n",
    "    output = (F.relu(input-lambd) * input) / (torch.abs(input - lambd) + epsilon)\n",
    "    return output\n",
    "\n",
    "\n",
    "class MemoryUnit(nn.Module):\n",
    "    def __init__(self, mem_dim, fea_dim, shrink_thres=0.0025):\n",
    "        super(MemoryUnit, self).__init__()\n",
    "        self.mem_dim = mem_dim\n",
    "        self.fea_dim = fea_dim\n",
    "        self.weight = Parameter(torch.Tensor(self.mem_dim, self.fea_dim))  # M x C\n",
    "        self.bias = None\n",
    "        self.shrink_thres= shrink_thres\n",
    "        # self.hard_sparse_shrink_opt = nn.Hardshrink(lambd=shrink_thres)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input):\n",
    "#         print(input.shape)\n",
    "#         print(self.weight.shape)\n",
    "        \n",
    "        att_weight = F.linear(input, self.weight)  # Fea x Mem^T, (TxC) x (CxM) = TxM\n",
    "        att_weight = F.softmax(att_weight, dim=1)  # TxM\n",
    "        # ReLU based shrinkage, hard shrinkage for positive value\n",
    "        if(self.shrink_thres>0):\n",
    "            att_weight = hard_shrink_relu(att_weight, lambd=self.shrink_thres)\n",
    "#             att_weight = F.softshrink(att_weight, lambd=self.shrink_thres)\n",
    "            # normalize???\n",
    "            att_weight = F.normalize(att_weight, p=1, dim=1)\n",
    "            # att_weight = F.softmax(att_weight, dim=1)\n",
    "            # att_weight = self.hard_sparse_shrink_opt(att_weight)\n",
    "        mem_trans = self.weight.permute(1, 0)  # Mem^T, MxC\n",
    "        output = F.linear(att_weight, mem_trans)  # AttWeight x Mem^T^T = AW x Mem, (TxM) x (MxC) = TxC\n",
    "        return {'output': output, 'att': att_weight}  # output, att_weight\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return 'mem_dim={}, fea_dim={}'.format(\n",
    "            self.mem_dim, self.fea_dim is not None\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ethical-canyon",
   "metadata": {},
   "outputs": [],
   "source": [
    "m=MemoryUnit(mem_dim=2000,fea_dim=1024, shrink_thres=0.0025)\n",
    "x=torch.rand((512,1024))\n",
    "out=m(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "swiss-raise",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([512, 1024]), torch.Size([512, 2000]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['output'].shape,out['att'].shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electric-costs",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "driven-fifty",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemModule_w_new(nn.Module):\n",
    "    def __init__(self, mem_dim, fea_dim, window,shrink_thres=0.0025, device='cuda'):\n",
    "        super(MemModule_w_new, self).__init__()\n",
    "        self.mem_dim = mem_dim\n",
    "        self.fea_dim = fea_dim\n",
    "        self.shrink_thres = shrink_thres\n",
    "        self.memory = MemoryUnit(self.mem_dim, self.fea_dim, self.shrink_thres)\n",
    "        self.window=window\n",
    "    def forward(self, input1):\n",
    "        s = input1.data.shape\n",
    "        x = input1.permute(0, 2, 3, 1).contiguous()\n",
    "        b,h,w,c=x.shape\n",
    "        num_window=int((h/self.window)*(w/self.window))\n",
    "        x=x.view(b*num_window,self.window,self.window,c)\n",
    "        x=x.view(x.size(0), -1)\n",
    "        \n",
    "        y_and = self.memory(x)\n",
    "        \n",
    "        y = y_and['output']\n",
    "        att = y_and['att']\n",
    "        y = y.view(s[0], s[2], s[3], s[1])\n",
    "        y = y.permute(0, 3, 1, 2)\n",
    "        att = att.view(s[0], int(h/self.window), int(w/self.window), self.mem_dim)\n",
    "        att = att.permute(0, 3, 1, 2)\n",
    "#         print(\"att.shape\",att.shape)\n",
    "        return {'output': y, 'att': att}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "internal-supervisor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 1024])\n",
      "torch.Size([512, 1024])\n",
      "att.shape torch.Size([512, 2000])\n",
      "torch.Size([2, 256, 32, 32])\n",
      "att.shape torch.Size([2, 2000, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "window=2\n",
    "fea_dim=window*window*c\n",
    "model1=MemModule(mem_dim=2000, fea_dim=1024,window=2)\n",
    "x1=torch.rand((2,256,32,32))\n",
    "out1=model1(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "stretch-berry",
   "metadata": {},
   "outputs": [],
   "source": [
    "#最终版\n",
    "class MemModule_w_new(nn.Module):\n",
    "    def __init__(self, mem_dim, fea_dim, window,c_size=1,shrink_thres=0.0025, device='cuda'):\n",
    "        super(MemModule_w_new, self).__init__()\n",
    "        self.mem_dim = mem_dim\n",
    "        self.fea_dim = fea_dim\n",
    "        self.shrink_thres = shrink_thres\n",
    "        self.memory = MemoryUnit(self.mem_dim, self.fea_dim, self.shrink_thres)\n",
    "        self.window=window\n",
    "        self.c_size=c_size\n",
    "    def forward(self, input1):\n",
    "        if self.c_size==1:\n",
    "            s = input1.data.shape\n",
    "            x = input1.permute(0, 2, 3, 1).contiguous()\n",
    "            b,h,w,c=x.shape\n",
    "            num_window=int((h/self.window)*(w/self.window))\n",
    "            x=x.view(b*num_window,self.window,self.window,c)\n",
    "            x=x.view(x.size(0), -1)\n",
    "\n",
    "            y_and = self.memory(x)\n",
    "\n",
    "            y = y_and['output']\n",
    "            att = y_and['att']\n",
    "            y = y.view(s[0], s[2], s[3], s[1])\n",
    "            y = y.permute(0, 3, 1, 2)\n",
    "            att = att.view(s[0], int(h/self.window), int(w/self.window), self.mem_dim)\n",
    "            att = att.permute(0, 3, 1, 2)\n",
    "    #         print(\"att.shape\",att.shape)\n",
    "            return {'output': y, 'att': att}\n",
    "        else:\n",
    "            B,C,H,W=input1.shape\n",
    "            input1=input1.view(B*self.c_size,-1,H,W)\n",
    "            s = input1.data.shape\n",
    "            x = input1.permute(0, 2, 3, 1).contiguous()\n",
    "            b,h,w,c=x.shape\n",
    "            num_window=int((h/self.window)*(w/self.window))\n",
    "            x=x.view(b*num_window,self.window,self.window,c)\n",
    "            x=x.view(x.size(0), -1)\n",
    "            y_and = self.memory(x)\n",
    "            y = y_and['output']\n",
    "#             print(y.shape)\n",
    "            att = y_and['att']\n",
    "            y = y.view(s[0], s[2], s[3], s[1])\n",
    "            y = y.permute(0, 3, 1, 2).contiguous()\n",
    "            y = y.view(B,C,H,W)\n",
    "            print(y.shape)\n",
    "            \n",
    "#             print(att.shape)\n",
    "            \n",
    "            att = att.view(B*self.c_size,int(h/self.window),int(w/self.window), self.mem_dim)\n",
    "            att = att.permute(0, 3, 1, 2)\n",
    "            print(att.shape)\n",
    "            return {'output': y, 'att': att}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "injured-wagner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 256, 32, 32])\n",
      "torch.Size([8, 2000, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "# window=2\n",
    "# fea_dim=window*window*c\n",
    "model1=MemModule_w_new(mem_dim=2000, fea_dim=1024,window=4,c_size=4)\n",
    "x1=torch.rand((2,256,32,32))\n",
    "out1=model1(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excited-adult",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "floral-property",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2048, 2, 2, 32])\n",
      "torch.Size([2048, 128])\n"
     ]
    }
   ],
   "source": [
    "x1=torch.rand((1,256,32,32))\n",
    "x=Flatten(x1,window=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strange-mileage",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "historic-brook",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemModule_ori(nn.Module):\n",
    "    def __init__(self, mem_dim, fea_dim, shrink_thres=0.0025, device='cuda'):\n",
    "        super(MemModule_ori, self).__init__()\n",
    "        self.mem_dim = mem_dim\n",
    "        self.fea_dim = fea_dim\n",
    "        self.shrink_thres = shrink_thres\n",
    "        self.memory = MemoryUnit(self.mem_dim, self.fea_dim, self.shrink_thres)\n",
    "\n",
    "    def forward(self, input):\n",
    "        s = input.data.shape\n",
    "        x = input.permute(0, 2, 3, 1)\n",
    "        print(x.shape)\n",
    "            \n",
    "        x = x.contiguous()\n",
    "        print(x.shape)\n",
    "        x = x.view(-1, s[1])\n",
    "        print(x.shape)\n",
    "        #\n",
    "        y_and = self.memory(x)\n",
    "        \n",
    "        #\n",
    "        y = y_and['output']\n",
    "        print('y',y.shape)\n",
    "        att = y_and['att']\n",
    "        print('att',att.shape)\n",
    "        \n",
    "        y = y.view(s[0], s[2], s[3], s[1])\n",
    "#         print( y.shape)\n",
    "        y = y.permute(0, 3, 1, 2)\n",
    "#         print( y.shape)\n",
    "        att = att.view(s[0], s[2], s[3], self.mem_dim)\n",
    "#         print( att.shape)\n",
    "        att = att.permute(0, 3, 1, 2)\n",
    "#         print( att.shape)\n",
    "\n",
    "        return {'output': y, 'att': att}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "suspected-bibliography",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 32, 32, 256])\n",
      "torch.Size([2, 32, 32, 256])\n",
      "torch.Size([2048, 256])\n",
      "y torch.Size([2048, 256])\n",
      "att torch.Size([2048, 2000])\n"
     ]
    }
   ],
   "source": [
    "model2=MemModule_ori(mem_dim=2000, fea_dim=256)\n",
    "x2=torch.rand((2,256,32,32))\n",
    "out2=model2(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "alpine-syracuse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 256, 32, 32]), torch.Size([2, 2000, 32, 32]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2['output'].shape,out2['att'].shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-brooklyn",
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_partition_c(x, window_size,c_size):\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C//c_size,c_size)\n",
    "    windows = x.permute(0, 1, 3,5, 2, 4, 6).contiguous().view(-1, window_size, window_size, c_size)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def window_reverse_c(windows, window_size,c_size,B, H, W,C):\n",
    "    x = windows.view(B, H // window_size, W // window_size, C // c_size, window_size, window_size,  c_size)\n",
    "    x = x.permute(0, 1, 4, 2, 5, 3,6).contiguous().view(B, H, W, C)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "certain-coupon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_partition(x, window_size):\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "    print(x.shape)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "flush-giving",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7820ad2ef6bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mwindows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow_partition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "x=torch.rand((2,32,32,256))\n",
    "windows=window_partition(x,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "handy-feelings",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096, 2, 2, 32])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aca95e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import math\n",
    " \n",
    "class test(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(test, self).__init__()\n",
    "#         self.up = nn.ConvTranspose2d(in_channels=1024, out_channels=512, \n",
    "#                                  kernel_size=2, stride=2)\n",
    "        self.down0 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    def forward(self, input):\n",
    "        x = self.down0(input)\n",
    "        \n",
    "        print(x.shape)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22725196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1024, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "model=test()\n",
    "y1=torch.rand((2,1024,32,32))\n",
    "res=model(y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "active-imagination",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import math\n",
    " \n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input,window):\n",
    "        s = input.data.shape\n",
    "        x = input.permute(0, 2, 3, 1).contiguous()\n",
    "        b,h,w,c=x.shape\n",
    "        num_window=int((h/window)*(w/window))\n",
    "        x=x.view(b*num_window,window,window,c)\n",
    "        print(x.shape)\n",
    "        \n",
    "        x=x.view(x.size(0), -1)\n",
    "        \n",
    "        print(x.shape)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "greater-manner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 2, 2, 256])\n",
      "torch.Size([512, 1024])\n",
      "torch.Size([128, 4, 4, 256])\n",
      "torch.Size([128, 4096])\n"
     ]
    }
   ],
   "source": [
    "y=Flatten()\n",
    "y1=torch.rand((2,256,32,32))\n",
    "y2=y(y1,2)\n",
    "y3=y(y1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "warming-swing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 1024])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "musical-shannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(2,2,4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "confirmed-affiliate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.8896, 0.8661, 0.4522, 0.0104],\n",
       "          [0.5813, 0.4983, 0.4351, 0.7579],\n",
       "          [0.3916, 0.9485, 0.3603, 0.1219],\n",
       "          [0.2412, 0.6517, 0.1153, 0.8062]],\n",
       "\n",
       "         [[0.3015, 0.7950, 0.1755, 0.8463],\n",
       "          [0.3675, 0.7933, 0.8266, 0.1903],\n",
       "          [0.1023, 0.0681, 0.2987, 0.1832],\n",
       "          [0.7480, 0.1329, 0.4124, 0.9361]]],\n",
       "\n",
       "\n",
       "        [[[0.0334, 0.8408, 0.6470, 0.8342],\n",
       "          [0.9630, 0.2236, 0.4736, 0.0899],\n",
       "          [0.6655, 0.0656, 0.5195, 0.2043],\n",
       "          [0.3681, 0.4142, 0.2742, 0.6806]],\n",
       "\n",
       "         [[0.6196, 0.1570, 0.0636, 0.9813],\n",
       "          [0.2829, 0.5748, 0.5487, 0.7993],\n",
       "          [0.0043, 0.8250, 0.7426, 0.7172],\n",
       "          [0.3696, 0.5365, 0.0301, 0.0208]]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "liquid-particular",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.8896, 0.8661],\n",
       "          [0.5813, 0.4983]],\n",
       "\n",
       "         [[0.3015, 0.7950],\n",
       "          [0.3675, 0.7933]]],\n",
       "\n",
       "\n",
       "        [[[0.0334, 0.8408],\n",
       "          [0.9630, 0.2236]],\n",
       "\n",
       "         [[0.6196, 0.1570],\n",
       "          [0.2829, 0.5748]]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:,:,:2,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-reward",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specialized-pierce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "earned-offer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_shrink_relu(input, lambd=0, epsilon=1e-12):\n",
    "    output = (F.relu(input-lambd) * input) / (torch.abs(input - lambd) + epsilon)\n",
    "    return output\n",
    "\n",
    "\n",
    "class MemoryUnit(nn.Module):\n",
    "    def __init__(self, mem_dim, fea_dim, shrink_thres=0.0025):\n",
    "        super(MemoryUnit, self).__init__()\n",
    "        self.mem_dim = mem_dim\n",
    "        self.fea_dim = fea_dim\n",
    "        self.weight = Parameter(torch.Tensor(self.mem_dim, self.fea_dim))  # M x C\n",
    "        self.bias = None\n",
    "        self.shrink_thres= shrink_thres\n",
    "        # self.hard_sparse_shrink_opt = nn.Hardshrink(lambd=shrink_thres)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input):\n",
    "        att_weight = F.linear(input, self.weight)  # Fea x Mem^T, (TxC) x (CxM) = TxM\n",
    "        att_weight = F.softmax(att_weight, dim=1)  # TxM\n",
    "        # ReLU based shrinkage, hard shrinkage for positive value\n",
    "        if(self.shrink_thres>0):\n",
    "            att_weight = hard_shrink_relu(att_weight, lambd=self.shrink_thres)\n",
    "#             att_weight = F.softshrink(att_weight, lambd=self.shrink_thres)\n",
    "            # normalize???\n",
    "            att_weight = F.normalize(att_weight, p=1, dim=1)\n",
    "            # att_weight = F.softmax(att_weight, dim=1)\n",
    "            # att_weight = self.hard_sparse_shrink_opt(att_weight)\n",
    "        mem_trans = self.weight.permute(1, 0)  # Mem^T, MxC\n",
    "        output = F.linear(att_weight, mem_trans)  # AttWeight x Mem^T^T = AW x Mem, (TxM) x (MxC) = TxC\n",
    "        return {'output': output, 'att': att_weight}  # output, att_weight\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return 'mem_dim={}, fea_dim={}'.format(\n",
    "            self.mem_dim, self.fea_dim is not None\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "completed-external",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NxCxHxW -> (NxHxW)xC -> addressing Mem, (NxHxW)xC -> NxCxHxW\n",
    "class MemModule1(nn.Module):\n",
    "    def __init__(self, mem_dim, fea_dim, shrink_thres=0.0025, device='cuda'):\n",
    "        super(MemModule1, self).__init__()\n",
    "        self.mem_dim = mem_dim\n",
    "        self.fea_dim = fea_dim\n",
    "        self.shrink_thres = shrink_thres\n",
    "        self.memory = MemoryUnit(self.mem_dim, self.fea_dim, self.shrink_thres)\n",
    "\n",
    "    def forward(self, input):\n",
    "        s = input.data.shape\n",
    "        x = input.permute(0, 2, 3, 1).contiguous()\n",
    "        print(x.shape)\n",
    "        x = x.view(-1, s[1])\n",
    "        print(x.shape)\n",
    "        #\n",
    "        y_and = self.memory(x)\n",
    "        \n",
    "        #\n",
    "        y = y_and['output']\n",
    "#         print('y',y.shape)\n",
    "        att = y_and['att']\n",
    "        print('att',att.shape)\n",
    "        \n",
    "        y = y.view(s[0], s[2], s[3], s[1])\n",
    "#         print( y.shape)\n",
    "        y = y.permute(0, 3, 1, 2)\n",
    "#         print( y.shape)\n",
    "        att = att.view(s[0], s[2], s[3], self.mem_dim)\n",
    "        print( att.shape)\n",
    "        att = att.permute(0, 3, 1, 2)\n",
    "        print( att.shape)\n",
    "\n",
    "        return {'output': y, 'att': att}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-owner",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1=MemModule1(mem_dim=512, fea_dim=256)\n",
    "x1=torch.rand((1,256,32,32))\n",
    "out1=model1(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-three",
   "metadata": {},
   "outputs": [],
   "source": [
    "out1['output'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chinese-pizza",
   "metadata": {},
   "outputs": [],
   "source": [
    "out1['att'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "built-sight",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statistical-smile",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NxCxHxW -> (NxHxW)xC -> addressing Mem, (NxHxW)xC -> NxCxHxW\n",
    "class MemModule_s(nn.Module):\n",
    "    def __init__(self, mem_dim, fea_dim, shrink_thres=0.0025, device='cuda'):\n",
    "        super(MemModule_s, self).__init__()\n",
    "        self.mem_dim = mem_dim\n",
    "        self.fea_dim = fea_dim\n",
    "        self.shrink_thres = shrink_thres\n",
    "        self.memory = MemoryUnit(self.mem_dim, self.fea_dim, self.shrink_thres)\n",
    "\n",
    "    def forward(self, x):\n",
    "        s = x.data.shape\n",
    "#         x = input.permute(0, 2, 3, 1).contiguous()\n",
    "        print(x.shape)\n",
    "        x = x.view(s[1]*s[0], -1)\n",
    "        print(x.shape)\n",
    "        #\n",
    "        y_and = self.memory(x)\n",
    "        \n",
    "        #\n",
    "        y = y_and['output']\n",
    "#         print('y',y.shape)\n",
    "        att = y_and['att']\n",
    "        print('att',att.shape)\n",
    "        \n",
    "        y = y.view(s[0], s[1], s[2], s[3])\n",
    "#         print( y.shape)\n",
    "#         y = y.permute(0, 3, 1, 2)\n",
    "#         print( y.shape)\n",
    "        att = att.view(s[0],s[1],self.mem_dim)\n",
    "#         print( att.shape)\n",
    "#         att = att.permute(0, 3, 1, 2)\n",
    "#         print( att.shape)\n",
    "\n",
    "        return {'output': y, 'att': att}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valuable-mapping",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=MemModule_s(mem_dim=512, fea_dim=64)\n",
    "x=torch.rand((32,1024,8,8))\n",
    "out=model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-candidate",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=MemModule(mem_dim=512, fea_dim=64)\n",
    "x=torch.rand((1,1024,8,8))\n",
    "out=model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-cosmetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "out['output'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-baseline",
   "metadata": {},
   "outputs": [],
   "source": [
    "out['att'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "diverse-pierre",
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_partition(x, window_size):\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "incorrect-block",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 32, 25])\n"
     ]
    }
   ],
   "source": [
    "y=torch.rand((256, 100))\n",
    "y=window_reverse(y,2,32,32)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "short-tissue",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NxCxHxW -> (NxHxW)xC -> addressing Mem, (NxHxW)xC -> NxCxHxW\n",
    "class MemModule_w(nn.Module):\n",
    "    def __init__(self, mem_dim, fea_dim, shrink_thres=0.0025,window_size=4, device='cuda'):\n",
    "        super(MemModule_w, self).__init__()\n",
    "        self.mem_dim = mem_dim\n",
    "        self.fea_dim = fea_dim\n",
    "        self.shrink_thres = shrink_thres\n",
    "        self.memory = MemoryUnit(self.mem_dim, self.fea_dim, self.shrink_thres)\n",
    "        self.window_size=window_size\n",
    "    def forward(self, input):\n",
    "        s = input.data.shape\n",
    "#         print(s)\n",
    "        x = input.permute(0, 2, 3, 1).contiguous()\n",
    "#         print(x.shape)\n",
    "        x=window_partition(x, self.window_size)###########\n",
    "#         print(x.shape)\n",
    "        b,w,w,c=x.shape\n",
    "        x = x.view(b, -1)\n",
    "#         x = x.view(-1, s[1])\n",
    "#         print(x.shape)\n",
    "        #\n",
    "        y_and = self.memory(x)\n",
    "        y = y_and['output']\n",
    "#         print('y',y.shape)\n",
    "        att = y_and['att']\n",
    "#         print('att',att.shape)\n",
    "        y=window_reverse(y,self.window_size,s[2], s[3])########\n",
    "#         print( y.shape)\n",
    "        y = y.permute(0, 3, 1, 2)\n",
    "#         print( y.shape)\n",
    "        att=window_reverse(att,self.window_size,s[2], s[3])\n",
    "        att = att.permute(0, 3, 1, 2)\n",
    "#         print( att.shape)\n",
    "\n",
    "        return {'output': y, 'att': att}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "norwegian-plymouth",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 16, 16, 4, 4, -1]' is invalid for input of size 51200",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-49b4cebc8e11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMemModule_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmem_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfea_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mout1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/YN/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-7049ac7e92bb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m#         print( y.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0matt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow_reverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0matt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m#         print( att.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-68ea0dcf62c8>\u001b[0m in \u001b[0;36mwindow_reverse\u001b[0;34m(windows, window_size, H, W)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwindow_reverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mwindow_size\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwindows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 16, 16, 4, 4, -1]' is invalid for input of size 51200"
     ]
    }
   ],
   "source": [
    "model1=MemModule_w(mem_dim=200, fea_dim=1024)\n",
    "x1=torch.rand((1,64,64,64))\n",
    "out1=model1(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surrounded-chapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1=MemModule1(mem_dim=100, fea_dim=256)\n",
    "x1=torch.rand((1,256,16,16))\n",
    "out1=model1(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turned-residence",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
