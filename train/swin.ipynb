{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sustainable-kentucky",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "from einops import rearrange\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "from swin_test import SwinTransformerBlock,BasicLayer_up,PatchMerging,PatchExpand,FinalPatchExpand_X4,BasicLayer,PatchEmbed,WindowAttention,Mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "charged-doctor",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "attempted relative import beyond top-level package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-faba39cc32a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_module\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMemModule_window\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMemModule\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMemModule1_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: attempted relative import beyond top-level package"
     ]
    }
   ],
   "source": [
    "from ..models.memory_module import MemModule_window,MemModule,MemModule1_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "super-cradle",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformerSys(nn.Module):\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=4, in_chans=3, num_classes=3,\n",
    "                 embed_dim=96, depths=[2, 2, 2, 2], depths_decoder=[1, 2, 2, 2], num_heads=[3, 6, 12, 24],\n",
    "                 window_size=7, mlp_ratio=4., qkv_bias=True, qk_scale=None,\n",
    "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n",
    "                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,\n",
    "                 use_checkpoint=False, final_upsample=\"expand_first\", **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        print(\"SwinTransformerSys expand initial----depths:{};depths_decoder:{};drop_path_rate:{};num_classes:{}\".format(depths,\n",
    "        depths_decoder,drop_path_rate,num_classes))\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ape = ape\n",
    "        self.patch_norm = patch_norm\n",
    "        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n",
    "        self.num_features_up = int(embed_dim * 2)\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.final_upsample = final_upsample\n",
    "\n",
    "        # split image into non-overlapping patches\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer if self.patch_norm else None)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        patches_resolution = self.patch_embed.patches_resolution\n",
    "        self.patches_resolution = patches_resolution\n",
    "\n",
    "        # absolute position embedding\n",
    "        if self.ape:\n",
    "            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "            trunc_normal_(self.absolute_pos_embed, std=.02)\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # stochastic depth\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
    "\n",
    "        # build encoder and bottleneck layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer),\n",
    "                               input_resolution=(patches_resolution[0] // (2 ** i_layer),\n",
    "                                                 patches_resolution[1] // (2 ** i_layer)),\n",
    "                               depth=depths[i_layer],\n",
    "                               num_heads=num_heads[i_layer],\n",
    "                               window_size=window_size,\n",
    "                               mlp_ratio=self.mlp_ratio,\n",
    "                               qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                               drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "                               drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n",
    "                               norm_layer=norm_layer,\n",
    "                               downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,\n",
    "                               use_checkpoint=use_checkpoint)\n",
    "            self.layers.append(layer)\n",
    "        \n",
    "        # build decoder layers\n",
    "        self.layers_up = nn.ModuleList()\n",
    "        self.concat_back_dim = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            concat_linear = nn.Linear(2*int(embed_dim*2**(self.num_layers-1-i_layer)),\n",
    "            int(embed_dim*2**(self.num_layers-1-i_layer))) if i_layer > 0 else nn.Identity()\n",
    "            if i_layer ==0 :\n",
    "                layer_up = PatchExpand(input_resolution=(patches_resolution[0] // (2 ** (self.num_layers-1-i_layer)),\n",
    "                patches_resolution[1] // (2 ** (self.num_layers-1-i_layer))), dim=int(embed_dim * 2 ** (self.num_layers-1-i_layer)), dim_scale=2, norm_layer=norm_layer)\n",
    "            else:\n",
    "                layer_up = BasicLayer_up(dim=int(embed_dim * 2 ** (self.num_layers-1-i_layer)),\n",
    "                                input_resolution=(patches_resolution[0] // (2 ** (self.num_layers-1-i_layer)),\n",
    "                                                    patches_resolution[1] // (2 ** (self.num_layers-1-i_layer))),\n",
    "                                depth=depths[(self.num_layers-1-i_layer)],\n",
    "                                num_heads=num_heads[(self.num_layers-1-i_layer)],\n",
    "                                window_size=window_size,\n",
    "                                mlp_ratio=self.mlp_ratio,\n",
    "                                qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                                drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "                                drop_path=dpr[sum(depths[:(self.num_layers-1-i_layer)]):sum(depths[:(self.num_layers-1-i_layer) + 1])],\n",
    "                                norm_layer=norm_layer,\n",
    "                                upsample=PatchExpand if (i_layer < self.num_layers - 1) else None,\n",
    "                                use_checkpoint=use_checkpoint)\n",
    "            self.layers_up.append(layer_up)\n",
    "            self.concat_back_dim.append(concat_linear)\n",
    "\n",
    "        self.norm = norm_layer(self.num_features)\n",
    "        self.norm_up= norm_layer(self.embed_dim)\n",
    "\n",
    "        if self.final_upsample == \"expand_first\":\n",
    "            print(\"---final upsample expand_first---\")\n",
    "            self.up = FinalPatchExpand_X4(input_resolution=(img_size//patch_size,img_size//patch_size),dim_scale=4,dim=embed_dim)\n",
    "            self.output = nn.Conv2d(in_channels=embed_dim,out_channels=self.num_classes,kernel_size=1,bias=False)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'absolute_pos_embed'}\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay_keywords(self):\n",
    "        return {'relative_position_bias_table'}\n",
    "\n",
    "    #Encoder and Bottleneck\n",
    "    def forward_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        if self.ape:\n",
    "            x = x + self.absolute_pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "        x_downsample = []\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x_downsample.append(x)\n",
    "            \n",
    "            x = layer(x)\n",
    "            print(\"x2.shape\",x.shape)\n",
    "\n",
    "        x = self.norm(x)  # B L C\n",
    "  \n",
    "        return x, x_downsample\n",
    "\n",
    "    #Dencoder and Skip connection\n",
    "    def forward_up_features(self, x, x_downsample):\n",
    "        for inx, layer_up in enumerate(self.layers_up):\n",
    "            if inx == 0:\n",
    "                x = layer_up(x)\n",
    "            else:\n",
    "                x = torch.cat([x,x_downsample[3-inx]],-1)###############\n",
    "                x = self.concat_back_dim[inx](x)\n",
    "                x = layer_up(x)\n",
    "\n",
    "        x = self.norm_up(x)  # B L C\n",
    "  \n",
    "        return x\n",
    "\n",
    "    def up_x4(self, x):\n",
    "        H, W = self.patches_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H*W, \"input features has wrong size\"\n",
    "\n",
    "        if self.final_upsample==\"expand_first\":\n",
    "            x = self.up(x)\n",
    "            x = x.view(B,4*H,4*W,-1)\n",
    "            x = x.permute(0,3,1,2) #B,C,H,W\n",
    "            print(\"x1.shape\",x.shape)\n",
    "            x = self.output(x)\n",
    "            \n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, x_downsample = self.forward_features(x)\n",
    "        x = self.forward_up_features(x,x_downsample)\n",
    "        print( \"x.shape\",x.shape)\n",
    "        x = self.up_x4(x)\n",
    "        print( \"x.shape\",x.shape)\n",
    "        down=[]\n",
    "        for i in range(len(x_downsample)):\n",
    "            b,wh,c=x_downsample[i].shape\n",
    "            x_downsample[i] = x_downsample[i].view(1,int(wh**0.5),int(wh**0.5),-1)\n",
    "#             y = y.permute(0,3,1,2) #B,C,H,W\n",
    "#             print(\"y.shape\",y.shape)\n",
    "        return x,x_downsample\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        flops += self.patch_embed.flops()\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            flops += layer.flops()\n",
    "        flops += self.num_features * self.patches_resolution[0] * self.patches_resolution[1] // (2 ** self.num_layers)\n",
    "        flops += self.num_features * self.num_classes\n",
    "        return flops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "speaking-elephant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SwinTransformerSys expand initial----depths:[2, 2, 2, 2];depths_decoder:[1, 2, 2, 2];drop_path_rate:0.1;num_classes:3\n",
      "---final upsample expand_first---\n",
      "x2.shape torch.Size([1, 784, 192])\n",
      "x2.shape torch.Size([1, 196, 384])\n",
      "x2.shape torch.Size([1, 49, 768])\n",
      "x2.shape torch.Size([1, 49, 768])\n",
      "x.shape torch.Size([1, 3136, 96])\n",
      "x1.shape torch.Size([1, 96, 224, 224])\n",
      "x.shape torch.Size([1, 3, 224, 224])\n",
      "output.shape torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    x=torch.randn(1,3,224,224)\n",
    "    model= SwinTransformerSys()\n",
    "    output,down=model(x)\n",
    "    print('output.shape',output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remarkable-singer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "convertible-bennett",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def window_partition(x, window_size):\n",
    "   \n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "   \n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-interpretation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
